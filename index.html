<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Zehao Zhang — Academic Website</title>
  <meta name="description" content="Zehao Zhang — CS undergraduate at Yonsei University. Computer Vision, Multimodal Modeling, Generative Vision Models, LLM/VLM."/>
  <style>
    :root{
      --bg:#0a0f16; --fg:#f8fbff; --muted:#d3e0f2; --accent:#5DE0E6;
      --card:#0f1830; --border:rgba(255,255,255,.16); --shadow:0 14px 40px rgba(0,0,0,.35);
    }
    @media (prefers-color-scheme: light){
      :root{ --bg:#f7fbff; --fg:#0b1726; --muted:#4a5a6b; --card:#ffffff; --border:rgba(0,0,0,.12); --shadow:0 14px 40px rgba(0,0,0,.08); }
    }
    *{box-sizing:border-box}
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);
      font-family:Inter,system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif;line-height:1.7;scroll-behavior:smooth}
    a{color:var(--fg);text-decoration:none}
    a:hover{opacity:.95}
    .container{max-width:1180px;margin:0 auto;padding:0 24px}

    /* nav */
    .nav{position:sticky;top:0;z-index:50;background:rgba(13,18,28,.72);
      backdrop-filter:blur(10px);border-bottom:1px solid var(--border)}
    .nav-inner{display:flex;align-items:center;justify-content:space-between;padding:12px 16px}
    .logo{font-weight:900;letter-spacing:.5px}
    .nav a{color:var(--fg);opacity:.92;margin-left:18px;font-size:1rem}
    .nav a:hover,.nav a.active{opacity:1}

    /* hero */
    .hero{position:relative;min-height:56vh;display:grid;place-items:center;overflow:hidden}
    .hero .inner{text-align:center;padding:58px 0;position:relative;z-index:2}
    .hero h1{font-size:clamp(40px,7vw,76px);margin:0 0 12px;letter-spacing:.6px;color:#fff;text-shadow:0 3px 16px rgba(20,40,80,.55),0 0 1px #fff;font-weight:900}
    .hero p{color:#fff;font-size:1.2rem;margin:12px auto 20px;max-width:880px;text-shadow:0 2px 12px rgba(0,0,0,.35)}
    .btns{display:flex;gap:12px;justify-content:center;margin-top:8px;flex-wrap:wrap}
    .btn{padding:11px 18px;border-radius:999px;background:var(--accent);color:#001018;font-weight:800;box-shadow:0 10px 28px rgba(93,224,230,.45);border:0;font-size:1rem}
    .btn.ghost{position:relative;background:rgba(255,255,255,.14);color:#fff;border:1.6px solid rgba(255,255,255,.85);font-weight:800;text-shadow:0 1px 6px rgba(0,0,0,.45);box-shadow:0 10px 28px rgba(0,0,0,.45),inset 0 0 0 9999px rgba(255,255,255,.05)}
    .btn.ghost:hover{background:rgba(255,255,255,.22);transform:translateY(-1px)}
    @media (prefers-color-scheme: light){ .btn.ghost{background:rgba(0,0,0,.08);color:#0b1726;border:1.6px solid rgba(0,0,0,.75);text-shadow:none;box-shadow:0 10px 24px rgba(0,0,0,.12),inset 0 0 0 9999px rgba(0,0,0,.03)} .btn.ghost:hover{background:rgba(0,0,0,.14)} }
    .anim-gradient{position:absolute;inset:-20%;background:
      radial-gradient(560px 360px at 22% 28%,rgba(93,224,230,.38),transparent 60%),
      radial-gradient(560px 360px at 78% 74%,rgba(161,113,255,.34),transparent 60%),
      linear-gradient(120deg,#0a1018,#0a111c); filter:blur(18px);animation:float 16s ease-in-out infinite alternate;z-index:0}
    @keyframes float{to{transform:translateY(-20px)}}

    section{padding:68px 0}
    h2{font-size:clamp(26px,3.2vw,38px);margin:0 0 18px}
    .muted{color:var(--muted)}
    .block{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:22px;margin:14px 0;box-shadow:var(--shadow)}
    .timeline{border-left:2px solid var(--border);padding-left:16px}
    .tl{margin:10px 0}

    /* about */
    .about-flex{display:flex;gap:22px;align-items:center;flex-wrap:wrap}
    .about-pic{width:180px;border-radius:14px;box-shadow:0 6px 20px rgba(0,0,0,.35);border:1px solid var(--border)}
    .about-text{flex:1 1 420px;min-width:300px;text-align:left}

    /* media blocks */
    .media{display:flex;gap:22px;align-items:flex-start;flex-wrap:wrap}
    .media-img{flex:0 0 360px;width:360px;max-width:100%;border-radius:12px;border:1px solid var(--border);box-shadow:0 8px 20px rgba(0,0,0,.4)}
    .media-body{flex:1 1 520px;min-width:300px}
    .row{display:flex;gap:16px;flex-wrap:wrap}
    .left{flex:0 0 160px;white-space:nowrap}  /* 日期始终一行 */
    .right{flex:1 1 auto}

    .reveal{opacity:0;transform:translateY(10px);transition:all .6s ease}
    .reveal.visible{opacity:1;transform:none}
    footer{padding:28px 0;border-top:1px solid var(--border);text-align:center;color:var(--muted)}
    @media (max-width: 860px){ .media-img{flex:1 1 100%;width:100%} .about-pic{width:160px} .left{flex:0 0 120px} }

    /* paper header */
    .paper-title{margin:0 0 6px;font-weight:900;font-size:1.15rem}
    .authors{margin:0 0 12px;color:var(--muted)}
  </style>
</head>

<body>
  <!-- NAV -->
  <header class="nav">
    <div class="nav-inner container">
      <a class="logo" href="#top">Zehao Zhang</a>
      <nav>
        <a href="#about">About Me</a>
        <a href="#education">Education</a>
        <a href="#publications">Publications</a>
        <a href="#practice">Research Experience</a>
        <a href="#internships">Industry Experience</a>
        <a href="#projects">Projects</a>
        <a href="#contact">Skills &amp; Interests</a>
      </nav>
    </div>
  </header>

  <!-- HERO -->
  <section id="top" class="hero">
    <div class="inner reveal">
      <h1>Zehao Zhang</h1>
      <p>CS undergraduate @ Yonsei University. Interests in <strong>Computer Vision</strong>,
         <strong>Large Language Models</strong>, <strong>Generative AI</strong>, and
         <strong>Reinforcement Learning</strong>.</p>
      <div class="btns">
        <a class="btn" href="mailto:taekho@yonsei.ac.kr">Email</a>
        <a class="btn" href="https://github.com/Fizzz9" target="_blank" rel="noreferrer">GitHub</a>
        <a class="btn" href="/assets/cv.pdf" target="_blank" rel="noreferrer">CV (PDF)</a>
      </div>
    </div>
    <div class="anim-gradient"></div>
  </section>

  <!-- ABOUT ME -->
  <section id="about">
    <div class="container reveal">
      <h2>About Me</h2>
      <div class="block">
        <div class="about-flex">
          <img src="/assets/img/1.jpg" alt="Zehao Zhang" class="about-pic">
          <div class="about-text">
            I work on <strong>computer vision</strong> and <strong>multimodal modeling</strong>, focusing on
            <strong>generative vision models</strong> and their integration with <strong>LLMs/VLMs</strong>.
            <br/><br/>
            My long-term goal is to build coherent <strong>vision–language systems</strong> that ground large models in
            geometry/video priors for consistent, editable generation while using language to organize and control perception,
            and to transfer these capabilities to embodied agents that perceive, plan, and act with human-interpretable control.
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- EDUCATION -->
  <section id="education">
    <div class="container reveal">
      <h2>Education</h2>
      <div class="timeline">
        <div class="tl">
          <strong>Yonsei University</strong>, B.S. in Computer Science
          <div class="muted">Mar. 2021 – Jan. 2026 (Expected) · Seoul, South Korea</div>
        </div>
      </div>
    </div>
  </section>

  <!-- PUBLICATIONS -->
  <section id="publications">
    <div class="container reveal">
      <h2>Publications</h2>
    <div class="block">
        <div class="media">
          <img src="/assets/img/2.png" alt="HumanGenesis Project" class="media-img">
          <div class="media-body">
            <div class="paper-title">HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics</div>
            <div class="authors">Weiqi Li, <strong>Zehao Zhang</strong>, Liang Lin, Guangrun Wang</div>
            <p>
              <strong>HumanGenesis</strong> develops a high-fidelity human synthesis pipeline that couples
              geometric reconstruction with generative refinement, enabling temporally consistent dynamic
              sequences from monocular inputs. A <em>Reflector</em> component introduces human-in-the-loop feedback
              to correct reconstruction drift and enhance perceptual fidelity, yielding stable, detailed renderings
              for animation and reenactment.
            </p>
            <div class="btns" style="justify-content:flex-start">
              <a class="btn ghost" href="https://liwq229.github.io/humangenesis" target="_blank" rel="noreferrer">Project Page</a>
              <a class="btn ghost" href="https://arxiv.org/abs/2508.09858" target="_blank" rel="noreferrer">arXiv</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESEARCH EXPERIENCE -->
  <section id="practice">
    <div class="container reveal">
      <h2>Research Experience</h2>
      <div class="block">
        <div class="media">
          <img src="/assets/img/3.png" alt="Attention-Conditional Diffusion (ACD)" class="media-img">
          <div class="media-body">
            <div class="row">
              <div class="left"><strong>2024.12 – 2025.09</strong></div>
              <div class="right">
                <strong>Attention-Conditional Diffusion for Video</strong><br/>
                <em>Research intern with Prof. Guangrun Wang (Sun Yat-sen University)</em>
                <p style="margin-top:8px">
                  We inject conditional control via supervised attention to improve semantic and spatial alignment,
                  and design a sparse 3D-aware layout with ControlNet plus automated annotation for interpretable control,
                  reducing artifacts and improving temporal consistency.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- INDUSTRY EXPERIENCE -->
  <section id="internships">
    <div class="container reveal">
      <h2>Industry Experience</h2>
      <div class="block">
        <div class="media">
          <img src="/assets/img/4.png" alt="Lenovo AI Lab Internship" class="media-img">
          <div class="media-body">
            <div class="row">
              <div class="left"><strong>2023.10 – 2024.01</strong></div>
              <div class="right">
                <strong>Research Intern — Lenovo AI Lab</strong><br/>
                Object-detection pipeline development in <em>PyTorch</em>. Conducted controlled comparisons of
                detector families and training recipes, analyzing accuracy–latency–memory trade-offs. Optimized
                data curation, augmentation, and deployment settings to improve throughput while preserving
                detection quality across device constraints.
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- PROJECTS (LLM / NLP) -->
  <section id="projects">
    <div class="container reveal">
      <h2>Projects (LLM / NLP)</h2>

      <div class="block">
        <div class="row">
          <div class="left"><strong>2025.03 – 2025.07</strong></div>
          <div class="right">
            <strong>Tiny Reasoning Agent (Distillation)</strong><br/>
            Distilled chain-of-thought and tool-use behaviors into tiny LMs under ReAct, improving sample efficiency
            and stability on multi-step reasoning tasks.
          </div>
        </div>
      </div>

      <div class="block">
        <div class="row">
          <div class="left"><strong>2024.11 – 2025.01</strong></div>
          <div class="right">
            <strong>Reduce Perplexity for LLaMA-3.2-1B</strong><br/>
            Continued pretraining on a large-scale Korean corpus; parameter-efficient tuning (LoRA) and prompting ablations
            to enhance linguistic robustness.
          </div>
        </div>
      </div>

      <div class="block">
        <div class="row">
          <div class="left"><strong>2024.09 – 2024.11</strong></div>
          <div class="right">
            <strong>ReAct Agents &amp; Prompting Strategies</strong><br/>
            Comparative study of Few-Shot, Self-Ask, Self-Consistency, and Self-Refine within ReAct on QA/interactive tasks;
            moderate structure improves stability, while excessive stacking can degrade performance.
          </div>
        </div>
      </div>

      <div class="block">
        <div class="row">
          <div class="left"><strong>2024.07 – 2024.08</strong></div>
          <div class="right">
            <strong>CoT vs. ToT Reasoning</strong><br/>
            Evaluated chain- vs tree-structured prompting on commonsense benchmarks; ToT shows broad gains while CoT remains
            competitive in low-data regimes.
          </div>
        </div>
      </div>

      <!-- LLaMA-2：精简到约两行 -->
      <div class="block">
        <div class="row">
          <div class="left"><strong>2024.05 – 2024.06</strong></div>
          <div class="right">
            <strong>Analysis &amp; Implementation of LLaMA-2 Features</strong><br/>
            Re-implemented Rotary PE, Pre-Norm, SwiGLU, RMSNorm, and GQA; linked each to a vanilla-Transformer weakness (positional aliasing, gradient flow, activation efficiency, stability, KV reuse) to clarify why these tweaks improve training and inference.
          </div>
        </div>
      </div>

    </div>
  </section>

  <!-- SKILLS & INTERESTS -->
  <section id="contact">
    <div class="container reveal">
      <h2>Skills &amp; Interests</h2>
      <div class="block">
        <p><strong>Skills.</strong> Languages: Python, C/C++, JavaScript/TypeScript. Frameworks: PyTorch, PyTorch Lightning, CUDA/TensorRT, OpenCV, FastAPI. Tooling: Linux, Git, Docker, tmux, ffmpeg, Weights&amp;Biases, HuggingFace, NVIDIA Nsight. Training: DDP/AMP, LoRA/PEFT; experiment/data pipelines and evaluation.</p>
        <p><strong>Interests.</strong> Travel, Music, Violin, League of Legends.</p>
      </div>
    </div>
  </section>

  <footer>
    <div class="container">© <span id="year"></span> Zehao Zhang · Academic Website.</div>
  </footer>

  <script>
    const obs = new IntersectionObserver((entries)=>entries.forEach(e=>{ if(e.isIntersecting) e.target.classList.add('visible') }), {threshold:.12});
    document.querySelectorAll('.reveal').forEach(el=>obs.observe(el));
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</body>
</html>
