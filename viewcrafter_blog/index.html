<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ViewCrafter Blog Post – CAS2105</title>
  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
    }

    h1 {
      font-size: 2rem;
      font-weight: 600;
      margin-bottom: 0.25rem;
      text-align: left;
    }

    h2 {
      font-size: 1.4rem;
      font-weight: 600;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.1rem;
      font-weight: 600;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img,
    figure video {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #000;
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    code {
      background: #eee;
      border-radius: 4px;
      padding: 0.15rem 0.4rem;
      font-size: 0.9rem;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    .meta {
      color: #666;
      font-size: 0.9rem;
      margin-bottom: 2rem;
      line-height: 1.4;
    }

    /* ===== Figure A / Figure B grid styling ===== */
    .grid-wrapper {
      border: 1px solid #ccc;
      border-radius: 12px;
      padding: 1rem;
      background: #fafafa;
      margin: 2rem 0;
    }

    .grid-title-row {
      display: grid;
      grid-template-columns: 1fr 1fr 1fr;
      text-align: center;
      font-size: 0.9rem;
      font-weight: 600;
      color: #444;
      margin-bottom: 0.75rem;
    }

    .grid-table {
      display: grid;
      grid-template-columns: 1fr 1fr 1fr;
      row-gap: 1.5rem;
      column-gap: 1rem;
      align-items: start;
      text-align: center;
    }

    .grid-cell img,
    .grid-cell video {
      max-width: 100%;
      border-radius: 8px;
      border: 1px solid #bbb;
      background: #000;
    }

    .cell-caption {
      font-size: 0.8rem;
      color: #666;
      margin-top: 0.4rem;
      line-height: 1.3;
    }

    .big-figcaption {
      font-size: 0.9rem;
      color: #555;
      line-height: 1.4;
      margin-top: 1rem;
      text-align: center;
    }

    .small-note {
      color: #777;
      font-size: 0.8rem;
      line-height: 1.3;
    }
  </style>
</head>
<body>

  <!-- Paper title figure -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="../assets/viewcrafter/title.png"
         alt="Paper title: ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      ViewCrafter (Yu et al., 2024). High-fidelity novel view synthesis from as little as one input image.
    </figcaption>
  </figure>

  <!-- Blog title / meta -->
  <h1>ViewCrafter: Walking a Camera Through a Scene Using Just One Photo</h1>
  <div class="meta">
    Zehao Zhang (Yonsei University)<bar>
    Oct 26, 2025 <bar>
    Based on: “ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis,”
    Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao,
    Tien-Tsin Wong, Ying Shan, and Yonghong Tian (2024).
  </div>


  <h2>0. First, what does ViewCrafter actually do?</h2>
  <p>
    Instead of starting with equations, let's just look at what this thing can do.
    ViewCrafter takes one single photo, plans a smooth camera path, and generates a
    video that looks like a real camera flying through the scene — with consistent
    geometry and realistic motion, not just a wobbly morph.
  </p>

  <!-- ===== Figure A: single-view demos ===== -->
  <figure class="grid-wrapper">
    <div class="grid-title-row">
      <div>Input single view</div>
      <div>Planned camera trajectory</div>
      <div>Generated fly-through (novel views)</div>
    </div>

    <div class="grid-table">

      <!-- Row 1: flower -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/flower.png" alt="single input view (flower scene)">
        <div class="cell-caption">Flower scene<br/>single captured photo</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/cflower.gif" alt="camera trajectory (flower scene)">
        <div class="cell-caption">Planned 6-DoF camera path</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/flower.gif" alt="generated fly-through (flower scene)">
        <div class="cell-caption">ViewCrafter's generated novel-view video (animated)</div>
      </div>

      <!-- Row 2: statue / building (wst) -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/wst.png" alt="single input view (statue/building scene)">
        <div class="cell-caption">Statue / building scene<br/>single captured photo</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/cwst.gif" alt="camera trajectory (statue/building scene)">
        <div class="cell-caption">Planned 6-DoF camera path</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/wst.gif" alt="generated fly-through (statue / building scene)">
        <div class="cell-caption">Generated fly-around sequence (animated)</div>
      </div>

      <!-- Row 3: train -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/train.png" alt="single input view (train scene)">
        <div class="cell-caption">Train scene<br/>single captured photo</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/ctrain.gif" alt="camera trajectory (train scene)">
        <div class="cell-caption">Planned 6-DoF camera path</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/train.gif" alt="generated fly-through (train scene)">
        <div class="cell-caption">Generated fly-by sequence (animated)</div>
      </div>

    </div>

    <figcaption class="big-figcaption">
      <b>Figure A.</b> Zero-shot novel view synthesis from a <b>single</b> photograph.
      Each row is one real scene. Left: the <b>only</b> real input image we give ViewCrafter.
      Middle: the automatically planned 6-DoF camera trajectory (how the virtual camera will move in 3D).
      Right: frames from the generated fly-through video following exactly that path.
      The motion is spatially meaningful, instead of “melting” the original picture.
      Images derived from Yu et al., 2024 / project demos.
    </figcaption>
  </figure>

  <p>
    With two input views (two different photos of the same scene), ViewCrafter fuses them
    into a stronger internal 3D understanding, so the synthesized motion gets even more
    consistent — fewer holes, fewer flickers, better geometry.
  </p>

  <!-- ===== Figure B: two-view demos ===== -->
  <figure class="grid-wrapper">
    <div class="grid-title-row">
      <div>Input view #1</div>
      <div>Input view #2</div>
      <div>Generated sweep (novel views)</div>
    </div>

    <div class="grid-table">

      <!-- Row 1: house -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house_1.png" alt="house scene - input view 1">
        <div class="cell-caption">House scene<br/>captured view #1</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house_2.png" alt="house scene - input view 2">
        <div class="cell-caption">Captured view #2</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house.gif" alt="generated sweep (house scene)">
        <div class="cell-caption">Generated smooth orbit (animated)</div>
      </div>

      <!-- Row 2: barn -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn_0.png" alt="barn scene - input view 1">
        <div class="cell-caption">Barn / yard scene<br/>captured view #1</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn_2.png" alt="barn scene - input view 2">
        <div class="cell-caption">Captured view #2</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn.gif" alt="generated sweep (barn scene)">
        <div class="cell-caption">Generated fly-through (animated)</div>
      </div>

      <!-- Row 3: car2 -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2_1.png" alt="car scene - input view 1">
        <div class="cell-caption">Car scene<br/>captured view #1</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2_2.png" alt="car scene - input view 2">
        <div class="cell-caption">Captured view #2</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2.gif" alt="generated sweep (car scene)">
        <div class="cell-caption">Generated walk-around (animated)</div>
      </div>

    </div>

    <figcaption class="big-figcaption">
      <b>Figure B.</b> Two-view fusion.
      In each row, the first two columns are just two casually captured viewpoints of the same scene.
      The third column shows ViewCrafter’s synthesized “camera sweep,” which smoothly moves between,
      around, and beyond those inputs while keeping texture and structure stable across frames.
      Images derived from Yu et al., 2024 / project demos.
    </figcaption>
  </figure>


  <h2>1. Why does this matter?</h2>
  <p>
    The task here is called <b>novel view synthesis (NVS)</b>:
    generate new viewpoints of a scene that the camera never actually captured.
    This is useful for real estate previews, AR/VR content creation,
    robotics perception, or just creative world-building.
  </p>
  <p>
    Classic 3D reconstruction methods (NeRF, Gaussian splatting, etc.)
    usually need lots of images from many angles.
    ViewCrafter attacks the <i>nightmare mode</i>: it tries to do high-fidelity,
    pose-controlled, temporally stable novel views from only one or two input images.
  </p>


  <h2>2. Key contributions (what’s really new?)</h2>
  <p>
    <b>(1) Point-conditioned video diffusion.</b>
    Instead of telling a diffusion model “please rotate 30° left,”
    ViewCrafter actually renders a coarse point cloud from that new camera pose,
    then asks the video diffusion model to clean it up.
    That gives precise camera control and less hallucination drift.
  </p>
  <p>
    <b>(2) Iterative exploration / next-best-view planning.</b>
    It doesn’t just spit out one fly-through.
    It repeatedly plans the next camera path that reveals unseen areas,
    synthesizes those views, and fuses them back to improve its global 3D.
  </p>
  <p>
    <b>(3) From almost nothing to an explorable 3D scene.</b>
    After several iterations, you get enough consistent multi-view frames
    to train a fast 3D Gaussian Splatting (3D-GS) model — so you can literally
    fly around the reconstructed scene in real time.
  </p>
  <p>
    <b>(4) Text → image → 3D world.</b>
    You can start from a text prompt, generate a single “fantasy” image,
    and then let ViewCrafter walk a virtual camera around that imagined world.
  </p>


  <h2>3. How does ViewCrafter actually work?</h2>

  <h3>Step 1. Build a rough point cloud from 1–2 images</h3>
  <p>
    ViewCrafter first estimates a colored point cloud and camera poses from just
    one or two input views (using dense stereo / depth prediction).
    Even with only a single photo, it can “fake” stereo to guess depth.
    The point cloud is incomplete and noisy, but it encodes geometry and pose.
  </p>

  <h3>Step 2. Render that point cloud from a new pose</h3>
  <p>
    We choose a target camera pose (move left, orbit around the statue, pan down, etc.).
    We then render the coarse point cloud from that exact pose. The render looks ugly,
    but it’s geometrically aligned with the desired viewpoint.
  </p>

  <h3>Step 3. Point-conditioned video diffusion</h3>
  <p>
    A video diffusion model takes those ugly rendered frames as conditioning
    and “beautifies” them into high-quality, temporally consistent video.
    Because the conditioning encodes the real 6-DoF camera pose, the generated
    clip follows that pose accurately.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img1.png"
         alt="ViewCrafter pipeline: point cloud reconstruction, point-conditioned video diffusion, iterative view synthesis." />
    <figcaption>
      Fig&nbsp;1 (Paper Fig.1). Overall pipeline.
      ViewCrafter builds a coarse point cloud from 1–2 reference views, renders it from planned
      camera poses, and feeds those renders to a point-conditioned video diffusion model to synthesize
      novel views. The process is iterative: each new clip updates the scene understanding and guides
      the next best camera trajectory. Image from Yu et al., 2024.
    </figcaption>
  </figure>

  <h3>Step 4. Iterative next-best-view planning</h3>
  <p>
    One short clip only covers a short path, so ViewCrafter acts like a tiny autonomous cameraman:
    it plans the next camera trajectory that will reveal missing areas
    (a “next best view”), generates a new clip for that path,
    and fuses those new views back into the scene model.
    Over time, it fills in the whole environment.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img2.png"
         alt="Trajectory planning vs fixed path: next-best-view exploration fills the scene more completely." />
    <figcaption>
      Fig&nbsp;2 (Paper Fig.8). Iterative camera trajectory planning.
      Instead of following one fixed path, ViewCrafter actively plans the next best camera view to
      uncover unseen regions. Compared to a predefined path, this exploration produces a denser,
      more complete reconstruction from extremely limited input. Image from Yu et al., 2024.
    </figcaption>
  </figure>


  <h2>4. What do we get at the end?</h2>

  <h3>4.1 A real-time 3D scene you can fly through</h3>
  <p>
    After several exploration loops, we’ve collected many consistent views and improved geometry.
    ViewCrafter then optimizes a 3D Gaussian Splatting (3D-GS) model from those views,
    which can be rendered in real time.
    Translation: from one or two casual photos, you end up with a scene you can actually navigate.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img3.png"
         alt="From sparse inputs to full 3D-GS: ViewCrafter synthesizes novel views, completes the point cloud, and supervises 3D Gaussian Splatting." />
    <figcaption>
      Fig&nbsp;3 (Paper Fig.2). From sparse inputs to an explorable 3D scene.
      ViewCrafter repeatedly synthesizes novel views and fuses them into a more complete point cloud,
      then uses those multi-view frames to supervise a 3D Gaussian Splatting model.
      The result is a 3D scene you can orbit in real time. Image from Yu et al., 2024.
    </figcaption>
  </figure>

  <h3>4.2 Text → image → multi-view → 3D</h3>
  <p>
    ViewCrafter can also “walk around” an imaginary scene that only exists in a text prompt:
    generate one reference image from text,
    then treat it like a real photo and synthesize multiple consistent new views,
    effectively giving you a mini 3D world from pure imagination.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img4.png"
         alt="Text-to-3D style results: astronaut in space, anime castle, blooming flower, ice cream." />
    <figcaption>
      Fig&nbsp;4 (Paper Fig.9). Text-to-3D style generation.
      Starting from just one AI-generated reference image (from a text prompt),
      ViewCrafter produces multiple consistent novel views,
      effectively “walking a camera” through an imagined world.
      This hints at prompt → explorable 3D scenes. Image from Yu et al., 2024.
    </figcaption>
  </figure>

  <h3>4.3 Pose control demo</h3>
  <p>
    The authors also show pose-accuracy demos:
    on the left you see the planned camera path (a full 6-DoF trajectory),
    and on the right you see the generated video following that exact path,
    compared to a baseline that drifts.
    This matters because many diffusion-based methods “float” and do not respect
    the requested camera motion.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img5.png"
         alt="Pose accuracy comparison: ViewCrafter vs Plücker Model baseline." />
    <figcaption>
      Fig&nbsp;5 (Paper Fig.7). Pose accuracy visualization.
      ViewCrafter’s generated novel-view video follows the intended 6-DoF camera trajectory
      (estimated pose vs ground truth is tight), while a Plücker-based baseline drifts more.
      This proves it’s not just making something that looks plausible — it is actually
      obeying the requested camera motion. Image from Yu et al., 2024.
    </figcaption>
  </figure>


  <h2>5. How does it compare to previous methods?</h2>
  <p>
    Against strong baselines like LucidDreamer, ZeroNVS, and MotionCtrl,
    ViewCrafter produces sharper details, fewer geometry glitches,
    and more stable textures across frames.
    Baselines tend to hallucinate, flicker, or warp objects,
    especially under big viewpoint changes.
  </p>
  <p>
    The paper also reports better quantitative metrics — PSNR and SSIM go up,
    LPIPS and FID go down (better perceptual quality),
    and the camera pose error is smaller.
    That means it’s not only pretty, it’s actually more faithful
    to the requested viewpoint.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img6.png"
         alt="Qualitative comparison: Reference, LucidDreamer, ZeroNVS, MotionCtrl, Ours, Ground Truth." />
    <figcaption>
      Fig&nbsp;6 (Paper Fig.3). Qualitative comparison.
      Each row shows novel-view synthesis from the same input photo across different systems.
      Baselines blur, distort geometry, or fail to match the intended pose.
      ViewCrafter stays sharper and more structurally consistent, closer to the ground truth.
      Image from Yu et al., 2024.
    </figcaption>
  </figure>

  <figure>
    <img src="../assets/viewcrafter/img7.png"
         alt="Quantitative table: Tanks-and-Temples, RealEstate10K, CO3D with LPIPS, PSNR, SSIM, FID, R_dist, T_dist." />
    <figcaption>
      Fig&nbsp;7 (Paper Table&nbsp;I / II). Quantitative results.
      On standard benchmarks (Tanks-and-Temples, RealEstate10K, CO3D),
      ViewCrafter achieves better perceptual quality (LPIPS↓, FID↓),
      better fidelity (PSNR↑, SSIM↑),
      and lower pose error (R<sub>dist</sub>, T<sub>dist</sub> ↓)
      compared to LucidDreamer, ZeroNVS, and MotionCtrl.
      Translation: it’s not just “looks nice,” it’s measurably closer
      to ground truth geometry and motion. Numbers from Yu et al., 2024.
    </figcaption>
  </figure>


  <h2>6. Limitations (what still breaks)</h2>
  <p>
    The authors are very honest that ViewCrafter is not magic:
  </p>
  <ul>
    <li><b>Extreme unseen viewpoints are still hard.</b><br/>
      If you only saw (say) the back of an object and demand a perfect close-up
      of the front, the model must hallucinate, and it can still fail.
    </li>
    <li><b>Garbage-in, garbage-out.</b><br/>
      ViewCrafter is surprisingly robust to noisy point clouds,
      but if the initial geometry is completely wrong,
      the final results inherit that mistake.
    </li>
    <li><b>Compute cost.</b><br/>
      Generating a video via diffusion is still expensive.
      For real-time navigation, they rely on converting the result
      into a fast 3D Gaussian Splatting model afterward.
    </li>
  </ul>

  <figure>
    <img src="../assets/viewcrafter/img8.png"
         alt="Point cloud renders vs PVDiffusion results, showing hole-filling and also the guesswork." />
    <figcaption>
      Fig&nbsp;8 (Paper Fig.5 + robustness discussion).
      Top row: raw point-cloud renders from novel viewpoints are fragmented, with missing regions and tearing.
      Bottom row: ViewCrafter’s point-conditioned video diffusion “repairs” these holes into plausible images.
      This shows both its strength (hallucinating consistent detail) and its limits
      (it is still guessing in truly unseen areas, and extreme viewpoints can break).
      Image from Yu et al., 2024.
    </figcaption>
  </figure>


  <h2>7. My takeaways</h2>
  <p>
    <b>Geometry as a contract.</b><br/>
    Instead of begging a diffusion model to “please follow the pose,”
    ViewCrafter hands it a crude point-cloud render from that exact pose and says:
    “Fix <i>this</i>.” That’s why the camera control is so accurate.
  </p>
  <p>
    <b>It behaves like an autonomous cameraman.</b><br/>
    The iterative next-best-view loop feels like a robot walking around
    a room, discovering new angles, and updating its map.
  </p>
  <p>
    <b>Text → explorable 3D is basically here.</b><br/>
    We’re getting extremely close to “type a prompt → get a little world
    you can literally shoot a camera inside.” That used to be science fiction.
  </p>
  <p>
    <b>They’re honest about limits.</b><br/>
    ViewCrafter doesn’t pretend this is solved forever.
    They clearly say where it fails (extreme unseen angles, heavy compute),
    which makes the work feel like serious research, not just marketing.
  </p>


  <footer>
    <h3>References / Figure credits</h3>
    <p>
      Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.-T.,
      Shan, Y., and Tian, Y.
      “<i>ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis.</i>”
      arXiv:2409.02048, 2024 (to appear in TPAMI 2025).
    </p>
    <p>
      All figures (Figure A, Figure B, Fig.1–8) and animations/gifs are derived
      from Yu et al. (2024) and the official ViewCrafter project demos.
      They illustrate: the overall pipeline (Fig.1),
      iterative camera planning (Fig.2),
      sparse-to-3DGS reconstruction (Fig.3),
      text-to-3D scene generation (Fig.4),
      pose alignment (Fig.5),
      qualitative comparisons (Fig.6),
      quantitative metrics (Fig.7),
      and robustness / limitations (Fig.8).
    </p>
    <p class="small-note">
      This blog post was prepared for CAS2105 Homework 3.
      Requirements: provide a public blog-style writeup, make it engaging,
      include visuals, and attach a PDF export.
    </p>
  </footer>

</body>
</html>
