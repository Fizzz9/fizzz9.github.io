<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ViewCrafter Blog Post – CAS2105</title>
  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
      font-weight: 600;
    }

    /* bigger title, tighter lines, try to keep it mostly on one line */
    h1 {
      font-size: 2.4rem;
      line-height: 1.22;
      max-width: 75ch;
      margin-bottom: 0.75rem;
      text-align: left;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img,
    figure video {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #fff;     /* keep white so no black pillars */
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    code {
      background: #eee;
      border-radius: 4px;
      padding: 0.15rem 0.4rem;
      font-size: 0.9rem;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    /* meta header under title */
    .meta {
      color: #444;
      font-size: 1rem;
      line-height: 1.5;
      margin-bottom: 2rem;
    }

    .meta-line-strong {
      font-weight: 600;
      color: #111;
    }

    /* ===== Figure A / Figure B grid styling ===== */
    .grid-wrapper {
      border: 1px solid #ccc;
      border-radius: 12px;
      padding: 1rem;
      background: #fafafa;
      margin: 2rem 0;
    }

    .grid-title-row {
      display: grid;
      grid-template-columns: 1fr 1fr 1fr;
      text-align: center;
      font-size: 0.9rem;
      font-weight: 600;
      color: #444;
      margin-bottom: 0.75rem;
    }

    .grid-table {
      display: grid;
      grid-template-columns: 1fr 1fr 1fr;
      row-gap: 1.5rem;
      column-gap: 1rem;
      align-items: start;
      text-align: center;
    }

    .grid-cell {
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .grid-cell img,
    .grid-cell video {
      width: 100%;
      max-height: 180px;
      object-fit: contain;
      border-radius: 8px;
      border: 1px solid #bbb;
      background: #fff;
    }

    .cell-caption {
      font-size: 0.8rem;
      color: #666;
      margin-top: 0.4rem;
      line-height: 1.3;
    }

    .big-figcaption {
      font-size: 0.9rem;
      color: #555;
      line-height: 1.4;
      margin-top: 1rem;
      text-align: center;
    }

    .small-note {
      color: #777;
      font-size: 0.8rem;
      line-height: 1.3;
    }

    .wide-figure {
      max-width: 850px;
      margin-left: auto;
      margin-right: auto;
    }
  </style>
</head>
<body>

  <!-- Paper title figure -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="../assets/viewcrafter/title.png"
         alt="Paper title: ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      ViewCrafter (Yu et al., 2024). High-fidelity novel view synthesis from as little as one or two input views.
    </figcaption>
  </figure>

  <!-- Blog title / meta -->
  <h1>ViewCrafter: Turning a Single Photo Into a Moving Camera Shot</h1>
  <div class="meta">
    <div class="meta-line-strong">Zehao Zhang (Yonsei University)</div>
    <div class="meta-line-strong">Oct 26, 2025</div>
    <div>
      Based on: “ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis,”
      Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao,
      Tien-Tsin Wong, Ying Shan, and Yonghong Tian (2024).
    </div>
  </div>

  <!-- ======================== SECTION 1 ======================== -->
  <h2>1. What problem is ViewCrafter solving?</h2>

  <p>
    Pretend you’re making a commercial. You took one nice picture of a flower. Now your boss says:
    “Cool. Can you make a dramatic 4-second shot where the camera glides around it like a nature
    documentary?” But you didn’t shoot any video. You never walked around the flower. You only
    have that <b>one</b> picture.
  </p>

  <p>
    From a computer vision point of view, that request is rude. A single photo normally cannot
    tell you what happens if you slide the camera left, tilt down, or orbit behind the object.
    Classic 3D methods (NeRF, Gaussian Splatting, etc.) usually want
    <b>many</b> views from <b>many</b> angles so they can actually triangulate depth.
  </p>

  <p>
    ViewCrafter’s mission is the nightmare version:  
    <b>Generate a realistic, smooth, controllable camera move video from only 1–2 input images,
    while actually respecting a 3D camera trajectory (full 6-DoF motion).</b>
    That means it’s not just “make new frames,” it’s “pretend we really moved a physical camera
    through this scene.”
  </p>

  <!-- ===== Figure A: single-view demos ===== -->
  <figure class="grid-wrapper">
    <div class="grid-title-row">
      <div>Input single view</div>
      <div>Planned camera path</div>
      <div>Generated fly-through</div>
    </div>

    <div class="grid-table">

      <!-- Row 1: flower -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/flower.png" alt="single input view (flower scene)">
        <div class="cell-caption">Flower scene<br/>Only this one photo is given.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/cflower.gif" alt="camera path (flower scene)">
        <div class="cell-caption">Planned 6-DoF orbit / push-in path.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/flower.gif" alt="generated fly-through (flower scene)">
        <div class="cell-caption">Generated fly-around video.</div>
      </div>

      <!-- Row 2: statue / building -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/wst.png" alt="single input view (statue/building scene)">
        <div class="cell-caption">Statue &amp; building<br/>Again, one captured view.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/cwst.gif" alt="camera path (statue scene)">
        <div class="cell-caption">Planned orbit around the statue.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/wst.gif" alt="generated fly-through (statue scene)">
        <div class="cell-caption">Generated plaza fly-around.</div>
      </div>

      <!-- Row 3: train -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/train.png" alt="single input view (train scene)">
        <div class="cell-caption">Train scene<br/>Single still frame.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/ctrain.gif" alt="camera path (train scene)">
        <div class="cell-caption">Planned side-tracking shot.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/train.gif" alt="generated fly-through (train scene)">
        <div class="cell-caption">Generated “walk-by” video.</div>
      </div>

    </div>

    <figcaption class="big-figcaption">
      Figure A. One input photo → ViewCrafter plans a 3D camera path → it generates a complete video
      that follows that path. The motion is not random morphing; it looks like an actual handheld
      or drone shot. (From Yu et al., 2024 demos.)
    </figcaption>
  </figure>

  <p>
    If you’re allowed two photos of the scene (e.g., you stepped to the side and snapped
    a second angle), things get even better. With two distinct viewpoints, ViewCrafter
    can build a stronger mental model of the 3D layout, which means fewer holes, more stable
    textures, and motion that feels less “melty.”
  </p>

  <!-- ===== Figure B: two-view demos ===== -->
  <figure class="grid-wrapper">
    <div class="grid-title-row">
      <div>Input view #1</div>
      <div>Input view #2</div>
      <div>Generated sweep</div>
    </div>

    <div class="grid-table">

      <!-- Row 1: house -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house_1.png" alt="house scene - input view 1">
        <div class="cell-caption">House / deck, view #1.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house_2.png" alt="house scene - input view 2">
        <div class="cell-caption">House / deck, view #2.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house.gif" alt="generated sweep (house scene)">
        <div class="cell-caption">Generated smooth patio orbit.</div>
      </div>

      <!-- Row 2: barn -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn_0.png" alt="barn scene - input view 1">
        <div class="cell-caption">Yard / barn, view #1.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn_2.png" alt="barn scene - input view 2">
        <div class="cell-caption">Yard / barn, view #2.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn.gif" alt="generated sweep (barn scene)">
        <div class="cell-caption">Generated fly-through between them.</div>
      </div>

      <!-- Row 3: car -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2_1.png" alt="car scene - input view 1">
        <div class="cell-caption">Parking-lot car, view #1.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2_2.png" alt="car scene - input view 2">
        <div class="cell-caption">Parking-lot car, view #2.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2.gif" alt="generated sweep (car scene)">
        <div class="cell-caption">Generated walk-around shot.</div>
      </div>

    </div>

    <figcaption class="big-figcaption">
      Figure B. Two casual views → smoother, more complete motion.  
      ViewCrafter fuses both inputs into a consistent internal 3D guess, then “films”
      a camera move that travels between and around them. (From Yu et al., 2024 demos.)
    </figcaption>
  </figure>


  <!-- ======================== SECTION 2 ======================== -->
  <h2>2. How does ViewCrafter actually work?</h2>

  <p>
    The high-level recipe is surprisingly understandable:
    <b>Guess a rough 3D world → plan a camera move → render what that camera
    “should” see → ask a video diffusion model to clean it up.</b>
    Underneath, each of those bold words hides real research.
  </p>

  <h3>Step 1. Reconstruct a rough point cloud</h3>
  <p>
    First, ViewCrafter tries to reverse-engineer some geometry from only 1–2 images.
    It estimates dense depth / stereo-like correspondences and recovers:
  </p>
  <ul>
    <li>a <b>colored point cloud</b> (lots of 3D points in space, each with color), and</li>
    <li>an approximate <b>camera pose</b> (where the original camera was, and which way it faced).</li>
  </ul>
  <p>
    With two input views, this is closer to standard multi-view stereo:
    you actually saw the object from different angles, so it can triangulate.
    With a single view, it has to “hallucinate” plausible depth based on learned priors
    (basically: it has seen a lot of similar scenes during training and guesses what’s 3D).
  </p>
  <p>
    Is the point cloud perfect? No. It’s messy, full of holes, and missing backsides.
    But it already encodes crucial spatial structure like
    “the statue stands in front of the building,”
    “this car is long along this axis,” etc.
  </p>

  <h3>Step 2. Plan a 6-DoF camera trajectory</h3>
  <p>
    Next, ViewCrafter decides how a <b>virtual camera</b> will move.
    Orbit around a statue? Dolly toward the flower? Slide past the train?
    This trajectory is full 6-DoF (3D translation + 3D rotation over time),
    so it’s not just “zoom in,” it’s “move like a real handheld or drone.”
  </p>
  <p>
    This is already different from many “animate my photo” apps, which just
    warp the pixels in place. ViewCrafter is literally planning a <i>camera path
    in 3D space</i>.
  </p>

  <h3>Step 3. Render the point cloud from that path</h3>
  <p>
    For every step along that path, ViewCrafter renders the rough point cloud
    from that exact viewpoint. These renders usually look terrible by themselves:
    background holes, floating chunks, torn edges.
  </p>
  <p>
    But here’s the trick: those ugly renders are <b>geometrically aligned</b>
    to the intended motion. They say, “If the camera were truly here in 3D,
    this is approximately what we’d see.”
  </p>

  <h3>Step 4. Point-conditioned video diffusion</h3>
  <p>
    Now the generative model enters. A video diffusion model takes those rough,
    holey renders and “beautifies” them into realistic, temporally consistent
    frames. It also conditions on reference appearance features (CLIP-style
    encoders etc.) so colors, textures, lighting, style, etc. stay consistent
    with the original input photo(s).
  </p>
  <p>
    The important part: the diffusion model is <b>not free to just freestyle</b>.
    It is forced to respect the geometry from Step 3. So the final video follows
    the planned 6-DoF path instead of drifting or sliding randomly.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img1.png"
         alt="Overall pipeline: point cloud reconstruction, point-conditioned video diffusion, iterative view synthesis." />
    <figcaption>
      Fig 1 (Paper Fig.1). Pipeline.  
      1) Build a point cloud + estimate pose.  
      2) Plan a future camera path.  
      3) Render the point cloud from those poses.  
      4) Use a video diffusion model to turn those rough renders into clean, realistic frames.
    </figcaption>
  </figure>

  <h3>Step 5. Iterate like an explorer (“next-best view”)</h3>
  <p>
    One little orbit only covers part of the world. So ViewCrafter repeats:
    it asks “Which areas are still missing?” and then plans a
    <b>next-best-view</b> path that would reveal those missing surfaces.
    Then it generates another short clip for that new path and fuses
    the new frames back into its global understanding.
  </p>
  <p>
    After several rounds, it behaves like a tiny autonomous cameraman / drone:
    scout → shoot → update the map → scout again. That loop is how it slowly
    “discovers” a whole scene from almost nothing.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img2.png"
         alt="Planned trajectory vs automatic next-best-view exploration." />
    <figcaption>
      Fig 2 (Paper Fig.8). Next-best-view planning.
      Instead of using one fixed camera path, ViewCrafter actively plans new motions
      to expose unseen regions. This gradually fills in geometry, even starting from
      extremely limited input.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 3 ======================== -->
  <h2>3. What do we get at the end?</h2>

  <h3>3.1 A scene you can actually explore in 3D</h3>
  <p>
    After multiple exploration loops, ViewCrafter has (1) a richer, more complete
    point cloud and (2) many synthesized viewpoints that are mutually consistent.
    At this point, it can train a fast 3D representation called
    <b>3D Gaussian Splatting (3D-GS)</b>.
  </p>
  <p>
    3D-GS represents the scene as a big cloud of tiny colored 3D Gaussians instead
    of classic polygons. The cool part: 3D-GS renders in real time. So once
    ViewCrafter has generated enough consistent novel views, you can supervise
    a 3D-GS model of your scene — and <b>now you can freely fly a virtual camera
    anywhere you want</b>.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img3.png"
         alt="From sparse inputs to 3D Gaussian Splatting supervision." />
    <figcaption>
      Fig 3 (Paper Fig.2). From sparse inputs → iterative completion → 3D-GS.
      ViewCrafter keeps generating and fusing novel views until it has enough to
      supervise a 3D Gaussian Splatting model. After that, you basically own
      an explorable 3D scene.
    </figcaption>
  </figure>

  <h3>3.2 Text → image → multi-view → mini world</h3>
  <p>
    There’s a sci-fi twist: you don’t even need real photos.  
    Step 1: write a text prompt (“an astronaut in space on a chair made of clouds,”
    “an anime-style castle,” “ice cream with orange slices,” etc.).  
    Step 2: use an image generator to get <i>one</i> high-quality still image.  
    Step 3: treat that single generated picture as if it were a “real photo” and
    run ViewCrafter on it.
  </p>
  <p>
    The result? ViewCrafter will “walk a camera” around a scene that never existed,
    giving you consistent new views of your imagination. That’s basically
    text → still image → multi-view → proto-3D.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img4.png"
         alt="Text-to-3D style generation: astronaut, anime castle, blooming flower, dessert." />
    <figcaption>
      Fig 4 (Paper Fig.9). Text-to-3D style results.
      Starting from just one AI-generated reference image (left side of each row),
      ViewCrafter produces multiple consistent viewpoints, as if you’re moving a
      camera around an imaginary object or environment.
    </figcaption>
  </figure>

  <h3>3.3 Pose accuracy: it follows the path you asked for</h3>
  <p>
    A common failure mode for “animate my photo” models is drift:
    you ask for “pan left,” but the output sort of wobbles in a dreamlike way.
  </p>
  <p>
    ViewCrafter is different. Because it conditions the diffusion model
    on explicit point-cloud renders for each planned pose, the final frames
    really obey the 6-DoF path. The authors show that the recovered camera
    trajectory matches the intended path more tightly than baselines
    such as Plücker-model camera parameterizations.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img5.png"
         alt="Pose accuracy comparison: ours vs Plücker model." />
    <figcaption>
      Fig 5 (Paper Fig.7). Pose alignment.
      The estimated camera pose (blue) stays close to ground truth versus a baseline
      that drifts. So ViewCrafter doesn’t just <i>look</i> like a camera move —
      it’s actually following the path you asked for.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 4 ======================== -->
  <h2>4. How good is it compared to previous work?</h2>

  <p>
    The paper compares ViewCrafter to strong baselines like LucidDreamer, ZeroNVS,
    and MotionCtrl. The side-by-side samples are kind of brutal:
    competing systems often smear textures, twist object shapes,
    or flicker between frames when the viewpoint changes a lot.
  </p>

  <p>
    ViewCrafter’s results stay sharper, the geometry looks more solid
    (a chair stays a chair instead of melting), and the motion better matches what
    you actually asked the camera to do.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img6.png"
         alt="Qualitative comparisons with LucidDreamer, ZeroNVS, MotionCtrl." />
    <figcaption>
      Fig 6 (Paper Fig.3). Qualitative comparison.
      Rows: statues, trains, indoor rooms, toys, etc.
      Columns: baseline methods vs. ViewCrafter vs. ground truth.
      Baselines often blur or warp; ViewCrafter stays structurally consistent
      and closer to reality.
    </figcaption>
  </figure>

  <p>
    And it’s not just visual. Quantitatively, ViewCrafter reports better perceptual
    quality (LPIPS↓, FID↓), better fidelity (PSNR↑, SSIM↑), and lower camera pose
    error (R<sub>dist</sub>, T<sub>dist</sub> ↓) across standard datasets like
    Tanks-and-Temples, RealEstate10K, and CO3D.
    Translation: it’s objectively closer to the real scene, not only “cool-looking.”
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img7.png"
         alt="Quantitative table across Tanks-and-Temples, RealEstate10K, CO3D." />
    <figcaption>
      Fig 7 (Paper Table I / II). Quantitative results.
      ViewCrafter outperforms prior methods on visual similarity (LPIPS, FID),
      reconstruction fidelity (PSNR, SSIM), and camera pose accuracy
      (R<sub>dist</sub>, T<sub>dist</sub>). In plain English: sharper, more faithful,
      more stable.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 5 ======================== -->
  <h2>5. Where does it still fail?</h2>

  <p>
    The authors are very honest: this is not magic, not a full digital twin
    of the universe. There are clear failure modes.
  </p>

  <ul>
    <li>
      <b>Extreme unseen viewpoints are hard.</b><br/>
      If you only saw the back of an object and demand a crystal-clear
      slow-motion close-up of the front, the system must hallucinate.
      It can guess, but sometimes it guesses wrong.
    </li>

    <li>
      <b>Garbage-in, garbage-out.</b><br/>
      The pipeline can tolerate noisy geometry,
      but if the initial point cloud is nonsense (depth is totally off,
      surfaces floating in space), that bad guess leaks into the final video.
    </li>

    <li>
      <b>Compute cost.</b><br/>
      Video diffusion is expensive.
      Generating a convincing clip is still way heavier than just “run a filter.”
      Their trick is: once you’ve generated enough views, convert the result to
      3D-GS, which <i>is</i> real-time. But getting there costs GPU.
    </li>
  </ul>

  <p>
    The figure below is basically “before vs. after makeup.” The top row shows the
    raw point-cloud renders from new viewpoints: holes, tearing, missing regions.
    The bottom row shows the diffusion model’s cleaned-up frames: smooth,
    filled-in surfaces. That’s great — but remember, in places the camera truly
    never saw, it is inventing.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img8.png"
         alt="Point cloud renders vs diffusion repaired frames." />
    <figcaption>
      Fig 8 (Paper Fig.5 / robustness discussion).
      Top row: direct point-cloud renders from novel viewpoints, full of gaps.
      Bottom row: ViewCrafter’s diffusion output fills those gaps with plausible
      texture. This is powerful, but also a reminder: in never-observed regions,
      it’s still guesswork.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 6 ======================== -->
  <h2>6. Mini technical recap</h2>

  <p>
    If we condense everything down to bullet points, the pipeline is:
  </p>

  <ol style="padding-left:1.25rem;">
    <li>
      <b>Sparse input:</b> 1–2 reference images of a real scene
      (or even one AI-generated “fake photo” from a text prompt).
    </li>

    <li>
      <b>Point cloud reconstruction:</b> estimate camera pose(s) and a colored
      point cloud via depth / stereo reasoning, even from minimal views.
    </li>

    <li>
      <b>Camera path planning:</b> choose a future 6-DoF camera trajectory
      (orbit, dolly, fly-by, walk-around). Later, choose <i>next-best-view</i>
      paths to uncover unseen regions.
    </li>

    <li>
      <b>Point-cloud rendering:</b> render the rough point cloud from every pose
      on that path. These previews are ugly but geometrically correct.
    </li>

    <li>
      <b>Point-conditioned video diffusion:</b> feed those previews plus
      learned appearance features into a video diffusion model, which produces
      clean, temporally consistent frames that follow the pose.
    </li>

    <li>
      <b>Iterative fusion:</b> add the new synthetic views back into the global
      scene model, improving coverage and consistency.
    </li>

    <li>
      <b>3D Gaussian Splatting supervision:</b> once you’ve got enough reliable,
      multi-view frames, train a 3D-GS model so you end up with an explorable,
      real-time 3D scene.
    </li>
  </ol>

  <p>
    In one sentence:
    <b>ViewCrafter glues geometry (point clouds + poses), planning (where to move next),
    and generative diffusion (fill in pixels) into an automated mini film crew
    that builds a world from almost nothing.</b>
  </p>


  <!-- ======================== SECTION 7 ======================== -->
  <h2>7. Why this work matters</h2>

  <p>
    The coolest mental shift in ViewCrafter is this:
    a single photo is no longer “just a picture.”
    It’s <i>the first keyframe of a world.</i>
    The system treats that photo like step zero of a scene it can now explore,
    film, and eventually convert into a navigable 3D model.
  </p>

  <p>
    Technically, the key trick is using geometry as a contract.
    Instead of asking a diffusion model, “Please move the camera nicely,”
    ViewCrafter says, “Here’s the exact 6-DoF path. Here’s a rough point-cloud
    render for each pose. Make <i>this</i> look real.”
    That contract is why the final video actually respects camera control
    and why you can later supervise a 3D-GS model with confidence.
  </p>

  <p>
    Practically, this is getting dangerously close to text-to-world:
    type a prompt → generate one nice still → ViewCrafter “walks a camera”
    around that imagined place → you end up with something explorable.
    That unlocks obvious applications in VR/AR content, robotics simulation,
    previsualizing movie shots, or even real-estate previews without doing
    a full scan.
  </p>

  <p>
    There are still limits — hallucination in totally unseen regions,
    and heavy GPU cost — but the direction is clear:
    we’re moving from “generate a cool frame” to
    <b>“generate a navigable place you can shoot inside.”</b>
  </p>


  <footer>
    <h3>References / Figure credits</h3>
    <p>
      Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.-T.,
      Shan, Y., and Tian, Y.
      “<i>ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis.</i>”
      arXiv:2409.02048, 2024 (to appear in TPAMI 2025).
    </p>
    <p>
      All figures (Figure A, Figure B, Fig.1–8) and gifs come from Yu et al. (2024)
      and the official ViewCrafter demos.  
      They illustrate: the full pipeline (Fig.1), next-best-view planning (Fig.2),
      iterative reconstruction → 3D Gaussian Splatting (Fig.3),
      text-to-multi-view generation (Fig.4),
      pose accuracy (Fig.5),
      qualitative comparisons (Fig.6),
      quantitative metrics (Fig.7),
      and robustness / failure cases (Fig.8).
    </p>
  </footer>

</body>
</html>
