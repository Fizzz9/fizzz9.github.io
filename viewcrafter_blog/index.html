<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ViewCrafter Blog Post – CAS2105</title>
  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
      font-weight: 600;
    }

    h1 {
      font-size: 2rem;
      margin-bottom: 0.5rem;
      text-align: left;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img,
    figure video {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #000;
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    code {
      background: #eee;
      border-radius: 4px;
      padding: 0.15rem 0.4rem;
      font-size: 0.9rem;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    .meta {
      color: #666;
      font-size: 0.9rem;
      margin-bottom: 2rem;
      line-height: 1.4;
    }

    /* ===== Figure A / Figure B grid styling ===== */
    .grid-wrapper {
      border: 1px solid #ccc;
      border-radius: 12px;
      padding: 1rem;
      background: #fafafa;
      margin: 2rem 0;
    }

    .grid-title-row {
      display: grid;
      grid-template-columns: 1fr 1fr 1fr;
      text-align: center;
      font-size: 0.9rem;
      font-weight: 600;
      color: #444;
      margin-bottom: 0.75rem;
    }

    .grid-table {
      display: grid;
      grid-template-columns: 1fr 1fr 1fr;
      row-gap: 1.5rem;
      column-gap: 1rem;
      align-items: start;
      text-align: center;
    }

    .grid-cell {
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .grid-cell img,
    .grid-cell video {
      width: 100%;
      max-height: 180px;      /* keep all three columns visually aligned */
      object-fit: contain;    /* scale inside the box instead of stretching */
      border-radius: 8px;
      border: 1px solid #bbb;
      background: #000;
    }

    .cell-caption {
      font-size: 0.8rem;
      color: #666;
      margin-top: 0.4rem;
      line-height: 1.3;
    }

    .big-figcaption {
      font-size: 0.9rem;
      color: #555;
      line-height: 1.4;
      margin-top: 1rem;
      text-align: center;
    }

    .small-note {
      color: #777;
      font-size: 0.8rem;
      line-height: 1.3;
    }

    .wide-figure {
      max-width: 850px;
      margin-left: auto;
      margin-right: auto;
    }
  </style>
</head>
<body>

  <!-- Paper title figure -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="../assets/viewcrafter/title.png"
         alt="Paper title: ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      ViewCrafter (Yu et al., 2024). High-fidelity novel view synthesis from as little as one or two input views.
    </figcaption>
  </figure>

  <!-- Blog title / meta -->
  <h1>ViewCrafter: Turning a Single Photo Into a Moving Camera Shot</h1>
  <div class="meta">
    Zehao Zhang (Yonsei University) · Oct 26, 2025 · CAS2105 Homework 3<br/>
    Based on: “ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis,”
    Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao,
    Tien-Tsin Wong, Ying Shan, and Yonghong Tian (2024).
  </div>

  <!-- ======================== SECTION 1 ======================== -->
  <h2>1. First, what is this thing?</h2>

  <p>
    Imagine you only have one photo of a flower, or a statue, or a train. You never walked around it.
    You never filmed a video. ViewCrafter tries to do something that sounds impossible:
    it plans how a virtual camera should move in 3D, and then it <b>generates the whole fly-through video</b>
    as if you actually filmed it.
  </p>

  <p>
    The model does not just “morph the image.” The camera motion is physically meaningful —
    you can orbit around the statue, slide past the train, or push in smoothly — even though
    the system only ever saw a single frame to start with.
  </p>

  <!-- ===== Figure A: single-view demos ===== -->
  <figure class="grid-wrapper">
    <div class="grid-title-row">
      <div>Input single view</div>
      <div>Planned camera path</div>
      <div>Generated fly-through</div>
    </div>

    <div class="grid-table">

      <!-- Row 1: flower -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/flower.png" alt="single input view (flower scene)">
        <div class="cell-caption">Flower scene<br/>Only this photo is given.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/cflower.gif" alt="camera path (flower scene)">
        <div class="cell-caption">Camera path (flower scene).</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/flower.gif" alt="generated fly-through (flower scene)">
        <div class="cell-caption">ViewCrafter’s fly-through video (generated).</div>
      </div>

      <!-- Row 2: statue / building -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/wst.png" alt="single input view (statue/building scene)">
        <div class="cell-caption">Statue / building scene<br/>One captured photo.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/cwst.gif" alt="camera path (statue scene)">
        <div class="cell-caption">Camera path (statue / building).</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/wst.gif" alt="generated fly-through (statue scene)">
        <div class="cell-caption">Generated orbit-style sequence.</div>
      </div>

      <!-- Row 3: train -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/train.png" alt="single input view (train scene)">
        <div class="cell-caption">Train scene<br/>One captured photo.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/ctrain.gif" alt="camera path (train scene)">
        <div class="cell-caption">Camera path (train).</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/train.gif" alt="generated fly-through (train scene)">
        <div class="cell-caption">Generated fly-by shot.</div>
      </div>

    </div>

    <figcaption class="big-figcaption">
      Figure A. One input photo → ViewCrafter plans a 3D camera path → it generates a whole video following that path.
      The motion is not random wobbling: it actually behaves like a camera moving in space.
      Images derived from Yu et al., 2024 / ViewCrafter demos.
    </figcaption>
  </figure>

  <p>
    With two views of the same place, it gets even better. If you give ViewCrafter two different angles
    (for example, two shots of the same house or car), it can fuse them into a better mental 3D model.
    That means smoother camera moves and fewer holes or glitches.
  </p>

  <!-- ===== Figure B: two-view demos ===== -->
  <figure class="grid-wrapper">
    <div class="grid-title-row">
      <div>Input view #1</div>
      <div>Input view #2</div>
      <div>Generated sweep</div>
    </div>

    <div class="grid-table">

      <!-- Row 1: house -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house_1.png" alt="house scene - input view 1">
        <div class="cell-caption">House scene<br/>View #1.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house_2.png" alt="house scene - input view 2">
        <div class="cell-caption">House scene<br/>View #2.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/house.gif" alt="generated sweep (house scene)">
        <div class="cell-caption">Generated orbit of the deck / yard.</div>
      </div>

      <!-- Row 2: barn -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn_0.png" alt="barn scene - input view 1">
        <div class="cell-caption">Yard / barn scene<br/>View #1.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn_2.png" alt="barn scene - input view 2">
        <div class="cell-caption">View #2.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/barn.gif" alt="generated sweep (barn scene)">
        <div class="cell-caption">Generated fly-through between them.</div>
      </div>

      <!-- Row 3: car -->
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2_1.png" alt="car scene - input view 1">
        <div class="cell-caption">Car scene<br/>View #1.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2_2.png" alt="car scene - input view 2">
        <div class="cell-caption">Car scene<br/>View #2.</div>
      </div>
      <div class="grid-cell">
        <img src="../assets/viewcrafter/car2.gif" alt="generated sweep (car scene)">
        <div class="cell-caption">Generated walk-around shot.</div>
      </div>

    </div>

    <figcaption class="big-figcaption">
      Figure B. Two casual photos → smoother, more complete camera motion. The model “fills in”
      the space between and around both inputs, and produces a stable sweep as if you filmed it yourself.
      Images derived from Yu et al., 2024 / ViewCrafter demos.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 2 ======================== -->
  <h2>2. How does it pull this off?</h2>

  <p>
    Here is the rough recipe. The high-level idea is:
    <b>guess a rough 3D world → plan a camera move → render what that camera “should” see → let a diffusion model clean it up.</b>
  </p>

  <h3>Step 1. Build a rough point cloud from 1–2 images</h3>
  <p>
    ViewCrafter first tries to recover geometry: where is the camera, where are surfaces, what’s the depth?
    From just one or two photos, it produces an incomplete point cloud (a coarse 3D scatter of colored points)
    and estimates initial camera poses. It’s not perfect, but it’s enough to understand “the statue is here,”
    “the ground is here,” etc.
  </p>

  <h3>Step 2. Plan a camera path in 6-DoF</h3>
  <p>
    The system then decides how a virtual camera should move. This is a real 6-DoF trajectory
    (position + rotation over time), like a drone path or a handheld orbit.
    It can plan simple moves (pan, dolly, orbit), or later, more exploratory paths that try
    to see parts of the scene that were never visible before.
  </p>

  <h3>Step 3. Render the rough point cloud from that path</h3>
  <p>
    If we take that rough point cloud and “look at it” from the new camera poses,
    we can render a very ugly preview of what each frame should look like.
    This preview has holes, missing background, weird floating chunks — but importantly,
    it is <b>geometrically aligned</b> with the desired viewpoint.
  </p>

  <h3>Step 4. Ask a video diffusion model to clean it up</h3>
  <p>
    Finally, a video diffusion model takes those ugly rendered previews and turns them into
    realistic, temporally consistent frames. Because the previews came from an exact 6-DoF path,
    the output video naturally follows that same path. This is how ViewCrafter keeps the camera
    under control instead of drifting.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img1.png"
         alt="Overall pipeline: point cloud reconstruction, point-conditioned video diffusion, iterative view synthesis." />
    <figcaption>
      Fig 1 (Paper Fig.1). The pipeline: build a rough point cloud, render it from a planned camera path,
      then let a video diffusion model “beautify” those renders into clean frames.
    </figcaption>
  </figure>

  <h3>Bonus: it doesn’t stop after one short clip</h3>
  <p>
    ViewCrafter can iterate. After generating one fly-through, it asks:
    “What part of the scene is still missing?” It then plans a new path that
    explores that missing area, generates another clip, and fuses that new
    information back into the world model.
  </p>

  <p>
    This is basically an automatic camera operator. Instead of you walking
    around the scene with a GoPro, the model “walks” for you.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img2.png"
         alt="Next-best-view planning vs fixed path: active exploration completes more of the scene." />
    <figcaption>
      Fig 2 (Paper Fig.8). It doesn’t just follow one fixed path.
      It actively plans “next best views” to reveal unseen areas and fill in missing geometry.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 3 ======================== -->
  <h2>3. From a couple of photos to an actual 3D scene</h2>

  <p>
    After a few rounds of exploring and generating new views, ViewCrafter has collected many
    consistent frames from many angles. At that point, it can train a fast 3D representation
    (3D Gaussian Splatting / 3D-GS) from those frames.
  </p>

  <p>
    That 3D-GS model can then be rendered in real time, like a lightweight game world.
    So yes: you start from one or two casual photos, and you end up with something you
    can literally fly around.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img3.png"
         alt="From sparse inputs to 3D-GS: synthesize new views, complete the point cloud, supervise a real-time splat model." />
    <figcaption>
      Fig 3 (Paper Fig.2). ViewCrafter keeps generating novel views,
      merges them into a denser point cloud, and finally trains a 3D Gaussian Splatting model
      you can explore in real time.
    </figcaption>
  </figure>

  <p>
    There is also a “text to world” angle: if you start with an AI-generated
    image from a text prompt (like “an astronaut sitting on a chair made of clouds”),
    ViewCrafter will treat that single image as if it were a real photo
    and try to walk a camera around that imaginary scene.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img4.png"
         alt="Text-to-3D style: astronaut in space, anime castle, flower, ice cream." />
    <figcaption>
      Fig 4 (Paper Fig.9). Start from a single AI-generated picture (from text),
      then generate multiple consistent views around that imaginary world.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 4 ======================== -->
  <h2>4. Precise camera control actually matters</h2>

  <p>
    A common problem in diffusion-based “make me a video” methods is that
    the camera kind of floats. You say “orbit around the statue,” but the
    model just hallucinates some drifting zoom that doesn’t match your request.
  </p>

  <p>
    ViewCrafter is different: because it conditions on the rendered point cloud
    for the <em>exact</em> planned trajectory, it can stick to the intended
    6-DoF motion. The authors visualize how well the generated camera
    matches the planned path.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img5.png"
         alt="Pose accuracy: ViewCrafter vs Plücker Model baseline." />
    <figcaption>
      Fig 5 (Paper Fig.7). The predicted camera motion from ViewCrafter
      lines up with the intended 6-DoF path much more tightly than a baseline.
      In other words, it obeys your shot direction.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 5 ======================== -->
  <h2>5. Is it actually better than older methods?</h2>

  <p>
    Yes, in two ways: how it looks, and how faithful it is.
  </p>

  <p>
    Visually, older systems like LucidDreamer, ZeroNVS, and MotionCtrl tend to blur details,
    stretch geometry, or flicker from frame to frame — especially when you try a big camera move.
    ViewCrafter keeps objects solid and consistent across time, and looks closer to
    the real held-out view.
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img6.png"
         alt="Qualitative comparison: Reference, LucidDreamer, ZeroNVS, MotionCtrl, Ours, Ground Truth." />
    <figcaption>
      Fig 6 (Paper Fig.3). Qualitative comparison.
      Baselines often warp or lose structure;
      ViewCrafter keeps shapes and textures stable and closer to ground truth.
    </figcaption>
  </figure>

  <p>
    Quantitatively, the paper reports better perceptual quality (LPIPS ↓, FID ↓),
    higher fidelity (PSNR ↑, SSIM ↑), and lower pose error (distance between the intended camera pose
    and what the model actually shows).
  </p>

  <figure class="wide-figure">
    <img src="../assets/viewcrafter/img7.png"
         alt="Quantitative table: Tanks-and-Temples, RealEstate10K, CO3D with LPIPS, PSNR, SSIM, FID, Rdist, Tdist." />
    <figcaption>
      Fig 7 (Paper Table I / II). On standard benchmarks (Tanks-and-Temples, RealEstate10K, CO3D),
      ViewCrafter scores better on both realism and geometric faithfulness.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 6 ======================== -->
  <h2>6. Where does it still break?</h2>

  <p>
    ViewCrafter is impressive, but it’s not sci-fi magic. The authors are pretty honest:
  </p>

  <ul>
    <li>
      <b>Extreme unseen angles are still guessing.</b><br/>
      If your single input photo only shows the front of an object
      and you demand a perfect close-up of the back, the system has to hallucinate.
      Sometimes it guesses wrong.
    </li>

    <li>
      <b>Noisy geometry in → noisy geometry out.</b><br/>
      The first point cloud can be messy. ViewCrafter can repair holes,
      but if the initial geometry is way off, the final video inherits that.
    </li>

    <li>
      <b>It’s not cheap to run.</b><br/>
      Generating a whole video with a diffusion model is still compute-heavy.
      The “real time” experience only comes after converting the result
      to a fast 3D Gaussian Splatting model.
    </li>
  </ul>

  <p>
    The figure below shows why we even need the diffusion step in the first place:
    the raw point-cloud render is full of gaps and broken regions, but the diffusion model
    fills those gaps into something that looks like a real frame.
  </p>

  <figure>
    <img src="../assets/viewcrafter/img8.png"
         alt="Point cloud renders vs cleaned-up diffusion results." />
    <figcaption>
      Fig 8 (Paper Fig.5 / robustness). Top row: raw point-cloud renders from new viewpoints are incomplete.
      Bottom row: ViewCrafter’s point-conditioned video diffusion fills in missing areas
      and produces coherent images — but it’s still guessing in places it has never actually seen.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 7 ======================== -->
  <h2>7. My takeaways</h2>

  <p>
    <b>It treats geometry like a contract.</b><br/>
    Instead of begging a diffusion model “please move left,”
    ViewCrafter literally hands it a noisy render from that exact leftward camera pose
    and says “make this look good.” That’s why the camera obeys.
  </p>

  <p>
    <b>It behaves like an autonomous cameraman.</b><br/>
    The system doesn’t just dream random motion. It plans where to go next,
    explores unseen areas, and keeps updating its 3D understanding.
    It’s basically doing scouting shots for you.
  </p>

  <p>
    <b>We’re really close to ‘type a prompt → get a little 3D world.’</b><br/>
    Starting from a text-generated single frame and then “walking” a camera through it
    already works surprisingly well. This is a huge step toward turning 2D generative models
    into explorable 3D content.
  </p>

  <p>
    <b>But it’s still guessing and still expensive.</b><br/>
    Total, perfect 3D from one image is not solved.
    ViewCrafter is just a very strong push in that direction —
    especially the part where the camera motion is actually under control.
  </p>


  <footer>
    <h3>References / figure credits</h3>
    <p>
      Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.-T.,
      Shan, Y., and Tian, Y.
      “<i>ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis.</i>”
      arXiv:2409.02048, 2024 (to appear in TPAMI 2025).
    </p>

    <p>
      All figures (Figure A, Figure B, Fig.1–8) and gifs are derived from Yu et al. (2024)
      and the official ViewCrafter project demos. They illustrate:
      (1) single-view and two-view results,
      (2) the pipeline and iterative next-best-view camera planning,
      (3) reconstruction into a 3D Gaussian Splatting scene,
      (4) text-to-3D-style generation,
      (5) pose control,
      (6) comparisons to prior work,
      (7) benchmark scores,
      and (8) limitations.
    </p>

    <p class="small-note">
      Public blog post for CAS2105 Homework 3.
      The homework asks for (a) a public blog-style explanation,
      (b) visual examples, and (c) a PDF export of this post.
    </p>
  </footer>

</body>
</html>
