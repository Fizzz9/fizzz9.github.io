<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>GS-DiT Blog Post – CAS2105</title>
  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
      font-weight: 600;
    }

    h1 {
      font-size: 2.2rem;
      line-height: 1.25;
      margin-bottom: 0.75rem;
      text-align: left;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img,
    figure video {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #fff;
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    code {
      background: #eee;
      border-radius: 4px;
      padding: 0.15rem 0.4rem;
      font-size: 0.9rem;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    /* meta header under title */
    .meta {
      color: #444;
      font-size: 1rem;
      line-height: 1.5;
      margin-bottom: 2rem;
    }

    .meta-line-strong {
      font-weight: 600;
      color: #111;
    }

    /* boxed comparison / grid sections */
    .grid-wrapper {
      border: 1px solid #ccc;
      border-radius: 12px;
      padding: 1rem;
      background: #fafafa;
      margin: 2rem 0;
    }

    .grid-title-row {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      text-align: center;
      font-size: 0.9rem;
      font-weight: 600;
      color: #444;
      margin-bottom: 0.75rem;
    }

    .grid-table {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      row-gap: 1.5rem;
      column-gap: 1rem;
      align-items: start;
      text-align: center;
    }

    .grid-cell {
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .grid-cell img,
    .grid-cell video {
      width: 100%;
      max-height: 180px;
      object-fit: contain;
      border-radius: 8px;
      border: 1px solid #bbb;
      background: #fff;
    }

    .cell-caption {
      font-size: 0.8rem;
      color: #666;
      margin-top: 0.4rem;
      line-height: 1.3;
    }

    .big-figcaption {
      font-size: 0.9rem;
      color: #555;
      line-height: 1.4;
      margin-top: 1rem;
      text-align: center;
    }

    .wide-figure {
      max-width: 850px;
      margin-left: auto;
      margin-right: auto;
    }

    /* side-by-side / 4-up video demos */
    .video-demo-block {
      margin: 2rem 0;
    }

    .video-demo-title {
      font-size: 1.1rem;
      font-weight: 600;
      text-align: center;
      margin-bottom: 1rem;
    }

    .video-row-2,
    .video-row-4 {
      display: grid;
      grid-gap: 1.5rem;
      align-items: start;
      justify-items: center;
    }

    .video-row-2 {
      grid-template-columns: repeat(2, 1fr);
    }

    .video-row-4 {
      grid-template-columns: repeat(4, 1fr);
    }

    .video-col {
      text-align: center;
      max-width: 100%;
    }

    .video-col video {
      width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #000;
    }

    .video-label {
      font-size: 0.85rem;
      color: #555;
      margin-top: 0.5rem;
    }

    .video-explainer {
      font-size: 0.9rem;
      color: #555;
      line-height: 1.4;
      text-align: center;
      max-width: 800px;
      margin: 1rem auto 2rem;
    }
  </style>
</head>
<body>

  <!-- Paper title figure -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="../assets/GS-DiT/title.png"
         alt="Paper title: GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      GS-DiT (Bian et al., 2025). Video generation with camera control, built on 3D point tracking and pseudo 4D Gaussian fields.
    </figcaption>
  </figure>

  <!-- Blog title / meta -->
  <h1>GS-DiT: Turning a Normal Video into a Controllable 3D Camera Shot</h1>
  <div class="meta">
    <div class="meta-line-strong">Zehao Zhang (Yonsei University)</div>
    <div class="meta-line-strong">Oct 27, 2025</div>
    <div>
      Based on: “GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking,”
      Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, and Hongsheng Li (2025).
    </div>
  </div>

  <!-- ======================== SECTION 0 ======================== -->
  <h2>0. Why do we care about this paper?</h2>

  <p>
    Imagine you record a short clip with your phone. The camera doesn’t move much.
    It’s just a normal “stand there and film” moment. But later you think:
    “I wish I had a nice orbit shot, or a dramatic dolly zoom, or even multiple camera angles,
    like a movie set.” The problem: you didn’t shoot any of that. You only have one boring clip.
  </p>

  <p>
    GS-DiT’s message is basically: <b>we can still get those shots</b>.
    We can create new camera motion after the fact. We can also edit an object in the scene
    (change how it looks) without ruining the rest of the video. We can even produce
    several different “virtual camera” versions from the same clip — like you had
    four cameras rolling, not one.
  </p>

  <p>
    This is only possible because GS-DiT tries to understand the clip as <b>3D over time</b>,
    not just as flat 2D frames. It builds what the authors call a <i>pseudo 4D Gaussian field</i>:
    roughly, a lightweight scene model in 3D that also changes over time (the 4th “D” is time).
    With that, the system can say, “OK, if I moved the camera here instead, what would I see?”
    and then generate a realistic video of that imaginary move.
  </p>

  <!-- A little hero demo from project page -->
  <div class="video-demo-block">
    <div class="video-demo-title">One input video → multiple controlled views</div>
    <div class="video-row-2">
      <div class="video-col" style="grid-column: 1 / span 2; max-width:600px;">
        <video controls src="../assets/GS-DiT/corgi.mov"></video>
        <div class="video-label">
          Project teaser (corgi scene). One handheld-style phone video.  
          GS-DiT produces several different camera angles / moves from that same clip.
        </div>
      </div>
    </div>
    <p class="video-explainer">
      The “corgi on the beach with sunglasses” demo from the official project page:
      same dog, same moment, but different camera paths and viewpoints, all synthesized.
      This is the cinematic dream — shoot once, direct later.
    </p>
  </div>

  <!-- ======================== SECTION 1 ======================== -->
  <h2>1. Quick background: What is a DiT and why is video hard?</h2>

  <p>
    DiT stands for <b>Diffusion Transformer</b>.  
    You’ve probably seen diffusion models like Stable Diffusion for images:
    they start from random noise and gradually “denoise” into an image.
  </p>

  <p>
    A standard DiT does that denoising using a Transformer backbone instead of a U-Net.
    It treats the noisy input (image, or even video frames in latent space) as a sequence
    of tokens, runs self-attention, and predicts how to remove noise step-by-step.
    You can also condition it on things like text (“a panda eating bamboo”),
    style tags, class labels, etc.  
    The original DiT formulation is described in work like “DiT: Diffusion Models with
    Transformers,” Peebles & Xie (2022) [arXiv:2212.09748].
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/dit.png"
         alt="Standard DiT block structure (Transformer-style diffusion denoiser with conditioning)." />
    <figcaption>
      Classic Diffusion Transformer (DiT).  
      Noise latents are turned into tokens, passed through Transformer blocks
      (self-attention, MLP, layer norms, conditioning), and progressively denoised.
      This works well for generating pretty frames.  
      But a vanilla DiT does <b>not</b> really know 3D geometry or the true camera pose.
      (Adapted from Peebles & Xie, 2022)
    </figcaption>
  </figure>

  <p>
    Why is that a problem for controllable video?  
    Because if you say “move the camera 20 cm to the left,” a plain DiT has no real idea
    what “20 cm” means in 3D space. It only knows pixels.  
    So it might just warp the image instead of simulating an actual camera motion.
  </p>

  <p>
    GS-DiT’s big promise is: “We’ll give the DiT an actual sense of 3D motion.”
  </p>

  <!-- ======================== SECTION 2 ======================== -->
  <h2>2. Core idea of GS-DiT</h2>

  <p>
    GS-DiT injects 3D understanding into the diffusion pipeline using two key pieces:
  </p>

  <ul>
    <li>
      <b>Dense 3D point tracking</b>:  
      Track thousands of points across the video, and estimate each point’s 3D position
      over time. So instead of just “this pixel moved,” we get
      “this physical point in the world moved from (x,y,z) to (x',y',z').”
    </li>

    <li>
      <b>Pseudo 4D Gaussian field</b>:  
      Using those tracked 3D points (plus depth) they build a scene representation
      made of little 3D Gaussian blobs that also evolve over time.
      It’s “pseudo 4D” — space (3D) + time (1D).
    </li>
  </ul>

  <p>
    Once you have that, you can do something magical:
    pick a brand new camera path (a sequence of 6-DoF poses:
    x/y/z position + rotation), “render” what that imaginary camera would see
    through the Gaussian field, and then ask a diffusion Transformer
    to clean it up into a realistic-looking video.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure1.png"
         alt="Pipeline: from input video → dense 3D point tracking → pseudo 4D Gaussian field → new camera render → diffusion cleanup." />
    <figcaption>
      Fig&nbsp;1 (Paper Fig.1). Overall pipeline.  
      Left: Input video.  
      Middle: the system builds a pseudo 4D Gaussian field using dense 3D point tracking
      and depth lifting. That’s basically the “world model.”  
      Right: for any camera path we want, GS-DiT renders a rough novel view and then
      uses diffusion to polish it into a realistic video. (Bian et al., 2025)
    </figcaption>
  </figure>

  <p>
    This makes camera control <i>explicit</i>.  
    We’re not just telling the model “look cooler please.”  
    We literally hand it a camera trajectory in 3D space and time.
  </p>

  <h3>Tracking quality actually matters</h3>

  <p>
    If the tracking is bad, everything collapses:
    the pseudo 4D field is broken, and the re-rendered view has holes.
    So the paper spends real effort on accurate dense tracking, and they compare
    against prior trackers on standard datasets.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/table1ang2.png"
         alt="Paper Tables 1 and 2: 2D and 3D point tracking benchmarks vs previous methods." />
    <figcaption>
      Tables&nbsp;1 &amp; 2 (Paper Tables 1 &amp; 2).  
      Quantitative tracking benchmarks such as TAPVid, TAPVid-3D, DriveTrack, Aria, etc.
      Metrics like AJ, OA, APD tell you how well points stay matched in 2D/3D.  
      Strong tracking is the foundation: if we cannot follow points in 3D,
      we cannot move the virtual camera believably. (Bian et al., 2025)
    </figcaption>
  </figure>

  <!-- ======================== SECTION 3 ======================== -->
  <h2>3. How GS-DiT is trained</h2>

  <p>
    Training GS-DiT is basically:  
    “Given an original video AND a view from a new camera pose,
    learn to generate the new view cleanly.”
  </p>

  <ul>
    <li><b>Input video</b>: the real clip you filmed.</li>
    <li><b>Condition video</b>: a synthetic novel-view clip rendered
        from the pseudo 4D Gaussian field using some chosen camera path.</li>
  </ul>

  <p>
    Both are encoded by a 3D VAE into latent features.
    These latents get concatenated and passed into a diffusion Transformer (DiT).
    Some parts are frozen, others are trained.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure3.png"
         alt="Training: encode input video and condition video to latent features, concatenate, feed to DiT denoiser." />
    <figcaption>
      Fig&nbsp;2 (Paper Fig.2). Training / conditioning.  
      Top: the “condition video” (the desired new camera view).  
      Bottom: the original input video.  
      Both go through a 3D VAE encoder. Their latent features are combined and then
      given to the DiT, which learns to denoise toward the correct target video.
      In short, the model learns how a requested camera path should actually look.
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <p>
    Another result of this setup is temporal + multi-view consistency:
    the same object should keep the same identity, texture, and pose,
    even as the camera swings around it. GS-DiT shows much stronger consistency
    than older “just hallucinate extra frames” methods.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure2.png"
         alt="Paper Fig.3: Otter demo with consistent look across different novel camera views and times." />
    <figcaption>
      Fig&nbsp;3 (Paper Fig.3). An otter character surfing.  
      GS-DiT generates new viewpoints over time while keeping appearance stable
      (same life jacket, same style) and making sure the camera motion follows
      the planned 3D path instead of wobbling randomly. (Bian et al., 2025)
    </figcaption>
  </figure>

  <!-- ======================== SECTION 4 ======================== -->
  <h2>4. What can GS-DiT actually do?</h2>

  <p>
    Now the fun part: we’ll look at real demos.
    Each sub-section starts with a normal input clip,
    then shows GS-DiT’s “what if the camera moved like THIS?” result.
  </p>

  <!-- 4.1 Camera pose control -->
  <h3>4.1 Camera pose control</h3>

  <p>
    We can literally say: “Tilt the camera down,” or “move the virtual camera
    to a new position,” and GS-DiT will render what that <em>new</em> camera
    should see. The scene is still the same (bubble on a frozen branch,
    desk clock on books) — just filmed from an angle that never existed
    in the original footage.
  </p>

  <div class="video-demo-block">
    <div class="video-demo-title">Camera Pose Control – Bubble Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/bubble.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/bubble-1b87f55dec310243.mp4"></video>
        <div class="video-label">Output Video (new camera pose)</div>
      </div>
    </div>
    <p class="video-explainer">
      Top example from the project page.  
      Left: your raw clip of an ice bubble sitting on a branch.  
      Right: GS-DiT’s “move / tilt the camera” version. The background
      and lighting still feel like the same place.
    </p>
  </div>

  <div class="video-demo-block">
    <div class="video-demo-title">Camera Pose Control – Clock Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/clock2.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/clock2-rotate_down_camera_poses_30_10.mp4"></video>
        <div class="video-label">Output Video (tilt / rotate down)</div>
      </div>
    </div>
    <p class="video-explainer">
      Bottom example from the project page.  
      The camera seems to “tilt down” toward the clock, but that motion never
      happened in the real recording — GS-DiT synthesized it.
    </p>
  </div>

  <!-- 4.2 Dolly zoom -->
  <h3>4.2 Dolly zoom</h3>

  <p>
    A <b>dolly zoom</b> is a classic film trick: the camera moves forward
    while zooming out, so the main subject stays almost the same size,
    but the background stretches in a dramatic, vertigo-style way.
    Usually you need careful camera rigging to pull that off.
  </p>

  <p>
    GS-DiT can fake that cinematic effect from a plain handheld-looking clip.
  </p>

  <div class="video-demo-block">
    <div class="video-demo-title">Dolly Zoom – Camel Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/camel2.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/camel2-id_camera_poses.mp4"></video>
        <div class="video-label">Output Video (dolly zoom style)</div>
      </div>
    </div>
    <p class="video-explainer">
      The camel stays calm and centered.  
      The background “pushes” and bends — that famous thriller-movie look —
      even though you never shot a true dolly rig. (Bian et al., 2025)
    </p>
  </div>

  <!-- 4.3 Object editing -->
  <h3>4.3 Object editing in video</h3>

  <p>
    GS-DiT can also edit one object in the scene while keeping everything else
    stable. For example, we can change how a toy windmill looks — its color,
    its texture — but keep the flowers, the depth feeling, and the camera motion
    consistent and believable.
  </p>

  <div class="video-demo-block">
    <div class="video-demo-title">Object Editing – Windmill Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/windmill.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/windmill-rotate_down_camera_poses_30_7.mp4"></video>
        <div class="video-label">Output Video (edited object)</div>
      </div>
    </div>
    <p class="video-explainer">
      Watch the spinning windmill blades.  
      Their style changes in the output, but the background and camera movement
      still feel natural. This is object-level control, not just a color filter.
    </p>
  </div>

  <!-- 4.4 Multi-camera shooting -->
  <h3>4.4 Multi-camera shooting from one clip</h3>

  <p>
    This is maybe the most “film studio” style result.
    Give GS-DiT just one drone-like coastal clip.
    It will produce several <em>different</em> camera trajectories of that same moment,
    almost like you had four cameras flying in formation.
  </p>

  <div class="video-demo-block">
    <div class="video-demo-title">Multi-camera Shooting – Coastal Scene</div>
    <div class="video-row-4">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur-rotate_left_camera_poses_30_20.mp4"></video>
        <div class="video-label">Output Trajectory 1 (rotate left)</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur-rotate_right_camera_poses_30_20.mp4"></video>
        <div class="video-label">Output Trajectory 2 (rotate right)</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur-rotate_downleft_camera_poses_30_20.mp4"></video>
        <div class="video-label">Output Trajectory 3 (rotate down-left)</div>
      </div>
    </div>
    <p class="video-explainer">
      One real clip in, many “virtual cameraman” views out.
      You can cut between these angles in editing, even though only one camera
      ever existed in the real world.
    </p>
  </div>

  <!-- ======================== SECTION 5 ======================== -->
  <h2>5. How does it compare to previous methods?</h2>

  <p>
    The paper compares GS-DiT with strong baselines like GCD and MonST3R.
    Older systems often break the scene when you try to move the camera:
    you get black holes where the model has no geometry,
    warped backgrounds, or subjects that fall apart.
  </p>

  <p>
    GS-DiT keeps the subject and environment more complete,
    and the invented new views look more like footage from a real moving camera.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure4.png"
         alt="Paper Fig.4: Qualitative comparison across scenes (swan, bike, car, cow, old man, otter)." />
    <figcaption>
      Fig&nbsp;4 (Paper Fig.4). Qualitative comparison.  
      Columns: original input video, recovered camera poses, GS-DiT output,
      plus baseline methods (GCD, MonST3R).  
      Rows: swan on water, biker by graffiti, street car, cow in a field,
      an old man, and the surfing otter.  
      GS-DiT fills in the world with fewer holes and less distortion. (Bian et al., 2025)
    </figcaption>
  </figure>

  <p>
    They also report numbers for multi-camera / multi-trajectory generation quality:
    higher PSNR and SSIM (closer to “ground truth”), and lower LPIPS (looks more similar
    to the real scene) than prior work.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/table3.png"
         alt="Paper Table 3: PSNR/SSIM/LPIPS comparison for multi-shooting video generation." />
    <figcaption>
      Table&nbsp;3 (Paper Table 3).  
      GS-DiT beats previous systems like MonST3R and GCD on PSNR↑, SSIM↑, LPIPS↓
      across datasets such as DAVIS, Sora, Pixabay.  
      Translation: sharper, more faithful, and more stable under new camera moves.
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <!-- ======================== SECTION 6 ======================== -->
  <h2>6. Mini technical recap</h2>

  <p>
    Let’s compress the pipeline into a short recipe:
  </p>

  <ol style="padding-left:1.25rem;">
    <li>
      <b>Start with your normal phone video.</b><br/>
      Track dense 3D points through time; estimate depth.
    </li>
    <li>
      <b>Build a pseudo 4D Gaussian field.</b><br/>
      This is the scene as 3D blobs that also evolve over time.
    </li>
    <li>
      <b>Pick any new camera path (6-DoF).</b><br/>
      Orbit, tilt, dolly zoom, rotate around subject, etc.
    </li>
    <li>
      <b>Render what that imaginary camera would see</b> from the Gaussian field.<br/>
      It’s rough, with holes, but it’s geometrically aligned with that path.
    </li>
    <li>
      <b>Feed that rough render into a Diffusion Transformer (DiT).</b><br/>
      The DiT denoises and fills in missing detail, producing a clean, realistic video.
    </li>
    <li>
      <b>Repeat for more paths or edits.</b><br/>
      Now you have multiple “virtual takes” of the same moment.
    </li>
  </ol>

  <p>
    In one line: GS-DiT glues together 3D point tracking, a pseudo 4D Gaussian “world,”
    and a diffusion Transformer, so you can <b>direct the camera after you’re done filming</b>.
  </p>

  <!-- ======================== SECTION 7 ======================== -->
  <h2>7. Why this work matters</h2>

  <p>
    A big shift in generative video is happening:
    we’re moving from “make a cool clip” to “give me the exact shot I want.”
    GS-DiT is a concrete step in that direction.
  </p>

  <p>
    Instead of thinking “this 10-second phone video is final,”
    GS-DiT treats it as raw material.  
    It reconstructs enough 3D structure that you can explore it,
    edit it, and reshoot it in post — with camera paths and effects
    that feel like real cinematography.
  </p>

  <p>
    This screams applications in:
  </p>

  <ul>
    <li><b>Filmmaking / previsualization:</b> plan fancy shots without renting gear.</li>
    <li><b>Object editing:</b> fix or restyle something in the scene without reshooting.</li>
    <li><b>AR / VR / robotics:</b> turn a casual scan into a little navigable world.</li>
  </ul>

  <p>
    Of course, it’s not perfect. If you never filmed the back side of an object,
    GS-DiT still has to hallucinate it. And diffusion models are heavy to run.
    But the direction is clear: <b>shoot once, direct later</b>.
  </p>

  <footer>
    <h3>References / Figure credits</h3>
    <p>
      Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., and Li, H.
      “<i>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking.</i>”
      arXiv:2501.02690, 2025.
    </p>
    <p>
      Peebles, W., and Xie, S.
      “<i>DiT: Diffusion Models with Transformers.</i>”
      arXiv:2212.09748, 2022.
    </p>
    <p>
      All figures (Fig.1–4) and tables (Tables 1–3) here are adapted from
      Bian et al. (2025) and the official GS-DiT project demos.
      They illustrate: dense 3D point tracking and pseudo 4D Gaussian fields (Fig.1),
      the training / conditioning design (Fig.2),
      appearance + motion consistency across views (Fig.3),
      qualitative comparisons to baselines (Fig.4),
      plus tracking metrics (Tables 1–2) and generation quality metrics (Table 3).
      Demo videos (bubble, clock, camel, windmill, coastal, corgi) come from the GS-DiT project page.
    </p>
  </footer>

</body>
</html>
