<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>GS-DiT Blog Post – CAS2105</title>

  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
      font-weight: 600;
    }

    h1 {
      font-size: 2.3rem;
      line-height: 1.22;
      max-width: 75ch;
      margin-bottom: 0.75rem;
      text-align: left;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #fff;
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    .meta {
      color: #444;
      font-size: 1rem;
      line-height: 1.5;
      margin-bottom: 2rem;
    }

    .meta-line-strong {
      font-weight: 600;
      color: #111;
    }

    .wide-figure {
      max-width: 850px;
      margin-left: auto;
      margin-right: auto;
    }

    /* simple table-like style for captions that mention metrics */
    .metric-note {
      font-size: 0.9rem;
      color: #444;
      text-align: center;
      line-height: 1.4;
      margin-top: -1rem;
      margin-bottom: 2rem;
    }

  </style>
</head>
<body>

  <!-- Paper title figure -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="assets/GS-DiT/title.png"
         alt="Paper title: GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking (Bian et al., 2025)" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      GS-DiT (Bian et al., 2025). Video generation with camera control, built on 3D point tracking and pseudo 4D Gaussian fields.
    </figcaption>
  </figure>

  <!-- Blog title / meta -->
  <h1>GS-DiT: Turning a Normal Video into a Controllable 3D Camera Shot</h1>
  <div class="meta">
    <div class="meta-line-strong">Zehao Zhang (Yonsei University)</div>
    <div class="meta-line-strong">Oct 27, 2025</div>
    <div>
      Based on: “GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields
      through Efficient Dense 3D Point Tracking,”
      Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang,
      and Hongsheng Li (2025).
      Also referencing Peebles &amp; Xie,
      “Scalable Diffusion Models with Transformers” (2022) for the base DiT design.
    </div>
  </div>

  <!-- Hero/demo figure from project page -->
  <figure class="wide-figure">
    <img src="assets/GS-DiT/corgi_grid.png"
         alt="GS-DiT multi-view style results on a dog-on-beach example: different virtual camera angles from a single casual video." />
    <figcaption>
      GS-DiT can treat one casual handheld clip almost like a tiny movie set:
      it can generate multiple believable camera angles / motions,
      even though you only filmed once.
      (Demo frames from the official GS-DiT project page, Bian et al. 2025.)
    </figcaption>
  </figure>

  <h2>0. Why do we care about this paper?</h2>
  <p>
    Imagine you filmed a short clip with your phone. The camera barely moved.
    But you want something that looks like a movie shot: a dolly zoom, a crane shot,
    an orbit around the subject, or even multiple different camera angles for editing.
    You did not actually shoot any of those moves.
  </p>

  <p>
    GS-DiT basically says: “That’s fine.
    I can synthesize those missing camera motions for you.
    I can also edit objects in your video, and I can even give you
    three or four different ‘virtual camera’ versions of the same moment,
    as if you had a multi-camera rig.”
  </p>

  <p>
    The key idea is that GS-DiT does <b>not</b> just warp pixels frame-by-frame.
    Instead, it tries to recover how the scene sits in 3D,
    track dense 3D points through time,
    and then control a virtual camera in <b>full 3D motion (6-DoF)</b>.
    That’s close to “4D video” — where you know how the scene evolves in space and time.
  </p>


  <h2>1. A quick background: what is DiT?</h2>

  <p>
    The base backbone here is DiT (“Diffusion Transformer”),
    first introduced as a vision/language diffusion model that replaces
    the usual U-Net with a pure Transformer block
    (Peebles &amp; Xie, 2022). In a normal diffusion model,
    you start with noise and iteratively denoise toward an image or video.
    DiT does this denoising in latent space using Transformer layers.
  </p>

  <p>
    Concretely, a DiT block:
  </p>
  <ul>
    <li>
      takes latent tokens (patchified latent features of the current noisy sample),
      plus conditioning information like timestep <i>t</i> or text prompt;
    </li>
    <li>
      runs self-attention and MLP layers, each modulated by adaptive layer norm
      (“adaLN-Zero”), which injects conditioning into the block;
    </li>
    <li>
      predicts how to nudge the latent away from noise and toward a clean result.
    </li>
  </ul>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/dit.png"
         alt="Classic DiT block design with adaLN-Zero and conditioning. Adapted from Peebles & Xie (2022)." />
    <figcaption>
      Classic DiT-style Transformer denoiser with adaptive layer norm (“adaLN-Zero”)
      to inject conditioning. (Diagram adapted from Peebles &amp; Xie, 2022.
      “Scalable Diffusion Models with Transformers.”)
      GS-DiT keeps this Transformer-based diffusion core, but extends it to 4D-style video.
    </figcaption>
  </figure>

  <p>
    So: vanilla DiT = “Transformer that denoises latents step-by-step.”
    GS-DiT = “DiT + 3D scene understanding + camera control.”
    Next we look at how they add the 3D / 4D part.
  </p>


  <h2>2. What is GS-DiT actually doing differently?</h2>

  <p>
    GS-DiT’s main contribution is to let a diffusion Transformer
    operate on something like a <b>pseudo 4D Gaussian field</b> of the scene.
    Roughly speaking:
  </p>

  <ul>
    <li>
      It densely tracks 3D points across the video (not just a few keypoints,
      but many points on surfaces).
    </li>
    <li>
      It lifts those tracked points, plus depth, to build a scene representation
      that behaves like a set of 3D Gaussians over time (a “pseudo 4D” field).
      Each Gaussian has a position in space, color/appearance info,
      and can be rendered from different viewpoints.
    </li>
    <li>
      Now we can render the scene from <b>new camera poses</b>
      — even ones that were never filmed.
    </li>
  </ul>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure2.png"
         alt="High-level pipeline: dense 3D point tracking, pseudo 4D Gaussian field, novel-view rendering, and conditioning of DiT." />
    <figcaption>
      (Figure from Bian et al., 2025.)
      Pipeline sketch: from the input clip, GS-DiT performs dense 3D point tracking,
      estimates depth, and reconstructs a pseudo 4D Gaussian field of the scene.
      This field can then be rendered from new camera poses (novel views),
      which serves as strong guidance for the diffusion model.
      The same mechanism also enables object edits and stylized re-renders.
    </figcaption>
  </figure>

  <p>
    The second part is how GS-DiT trains the diffusion Transformer
    to actually follow these new camera views.
    They feed both the original view and a “condition” view
    (like a target camera motion or edited object) into the model.
    Both are encoded into latent features with a 3D video VAE,
    and then concatenated / injected into a DiT-style denoiser.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure3.png"
         alt="Training / conditioning setup: encode input video and condition video with a 3D VAE, then guide DiT to generate the target output." />
    <figcaption>
      (Figure from Bian et al., 2025.)
      Training / conditioning diagram.
      Top branch: “condition video,” e.g. a clip rendered from a new camera pose,
      or with an edited object.
      Bottom branch: the original input clip.
      A frozen 3D VAE encodes both into latent features,
      which are concatenated and fed to the DiT denoiser.
      GS-DiT learns to generate the output that matches the requested camera motion
      or edit, while staying consistent with the scene.
    </figcaption>
  </figure>


  <h2>3. How does it get 3D without multi-camera rigs?</h2>

  <p>
    Normally, to reconstruct a scene in 3D you would shoot from many angles.
    GS-DiT does not assume that.
    Instead, it relies on <b>dense 3D point tracking</b>:
    it tries to follow thousands of points on surfaces across frames,
    and also estimates depth for those points.
    This gives approximate camera poses and scene geometry over time —
    enough to guide novel-view rendering.
  </p>

  <p>
    That tracking quality really matters. If you cannot robustly follow points
    in 3D, any “virtual camera move” will just smear or break.
    The paper benchmarks their point tracking against previous methods
    like TAPIR, CoTracker, SpatialTracker, etc.,
    on datasets such as TAPVid (2D tracking) and TAPVid-3D (3D tracking).
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/table1and2.png"
         alt="Table 1 and Table 2: comparison of dense point tracking performance in 2D and 3D vs. prior work." />
    <figcaption>
      Tables 1 &amp; 2 (Bian et al., 2025).
      GS-DiT’s tracking pipeline (their “Ours + ...” rows)
      is competitive or better on dense 2D / 3D point tracking benchmarks.
      Better tracking → better geometry → better camera control later.
    </figcaption>
  </figure>


  <h2>4. What can GS-DiT actually do?</h2>

  <p>
    Now the fun part. Once you have (1) dense tracked 3D points,
    (2) pseudo 4D Gaussian fields you can render from new viewpoints,
    and (3) a DiT-style diffusion model trained to respect those views,
    you can do things that look like pro cinematography using only
    a casual input clip.
  </p>

  <h3>4.0 Novel camera motion from a single shot</h3>
  <p>
    GS-DiT can generate smooth camera paths that were never filmed,
    while keeping the subject and background consistent.
    The paper shows scenes like an otter surfing:
    rows correspond to different novel camera trajectories,
    each following a planned 3D path.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure1.png"
         alt="Figure 1: multiple synthetic camera paths for an otter surfing clip. Each row shows new view trajectories the original video never had." />
    <figcaption>
      Figure 1 (Bian et al., 2025).
      Input video on top. Below: several synthesized camera trajectories.
      Each new row is GS-DiT’s “virtual camera,” moving along a planned 3D path.
      The model keeps the otter, the board, and the water coherent
      even for views and motions the phone never captured.
    </figcaption>
  </figure>

  <h3>4.1 Camera pose control</h3>
  <p>
    We can say “tilt the camera down a bit,” or
    “move the camera to a new viewpoint,”
    and GS-DiT will synthesize frames from that new pose.
    The content should still look like the same real scene.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/cam_pose_control.png"
         alt="Camera pose control examples: bubble scene and clock scene, input vs output." />
    <figcaption>
      Camera Pose Control (project demo). Top: an ice bubble on a branch.
      Bottom: a clock on books.
      Left is the original input video.
      Right is GS-DiT’s output with a controlled camera tilt / move.
      The scene appearance stays realistic, not just warped.
      (Demo frames from the official GS-DiT project page.)
    </figcaption>
  </figure>

  <h3>4.2 Dolly zoom</h3>
  <p>
    The “dolly zoom” is a classic film trick:
    the camera physically moves in toward the subject
    while zooming out the lens,
    so the subject size stays similar
    but the background stretches in a dramatic way.
  </p>

  <p>
    GS-DiT can synthesize that effect from a simple handheld animal clip —
    even though you never actually did a cinematic dolly move with a rig.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/dolly_zoom.png"
         alt="Dolly zoom demo with a camel: input vs GS-DiT output." />
    <figcaption>
      Dolly Zoom (project demo).
      Left row: the plain handheld-looking camel clip.
      Right row: GS-DiT’s output with a dolly-zoom style motion.
      The camel remains stable while the background “pushes.”
      (Demo frames from the official GS-DiT project page.)
    </figcaption>
  </figure>

  <h3>4.3 Object editing in video</h3>
  <p>
    GS-DiT can also edit an object in the video while keeping everything else
    consistent (geometry, lighting, background).
    For example, change the color / texture / style of a toy windmill,
    while the camera feel and rest of the scene remain natural.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/obj_edit.png"
         alt="Object editing: windmill blades appearance changed between input and output." />
    <figcaption>
      Object Editing (project demo).
      Only the foreground windmill is modified.
      The background, camera motion, and depth cues still match.
      (Demo frames from the official GS-DiT project page.)
    </figcaption>
  </figure>

  <h3>4.4 Multi-camera shooting from one clip</h3>
  <p>
    This one feels like a real film studio trick.
    You feed GS-DiT one drone-like coastal clip,
    and it produces multiple “virtual camera” versions
    — for example: rotate left, rotate right, rotate down-left.
  </p>

  <p>
    That means you basically get four different angles
    of the same exact moment,
    as if you had four cameras rolling at once.
    Super useful for editing / cutting between angles.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/multi_cam.png"
         alt="Multi-camera shooting demo: one coastal clip, plus three synthesized trajectories." />
    <figcaption>
      Multi-camera Shooting (project demo).
      Far left: the original coastal aerial/drone clip.
      Then three different GS-DiT “virtual camera” trajectories
      (rotate left, rotate right, rotate down-left).
      You can now cut between angles that never existed in the raw footage.
      (Demo frames from the official GS-DiT project page.)
    </figcaption>
  </figure>


  <h2>5. How does GS-DiT compare to previous work?</h2>

  <p>
    Prior work like MonST3R and GCD can generate some novel views from a single video,
    but they often struggle to keep geometry stable,
    especially with fast motion, moving backgrounds, or big camera shifts.
  </p>

  <p>
    GS-DiT tries to fix this with two ideas:
    (1) dense 3D point tracking → better 3D structure,
    (2) conditioning a diffusion Transformer on rendered
    novel views from the pseudo 4D Gaussian field → tighter camera control.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure4.png"
         alt="Qualitative comparison: Input Video, Camera Poses, Ours, GCD, MonST3R across several scenes like swan, cyclist, car, cow..." />
    <figcaption>
      Figure 4 (Bian et al., 2025). Qualitative comparison.
      Columns show: input frames, recovered camera poses, GS-DiT,
      and two strong baselines (GCD, MonST3R).
      Rows: different scenes — a swan on water, a cyclist, traffic, animals, human portraits, etc.
      Baselines often show tearing / missing regions or black voids for unseen areas.
      GS-DiT produces cleaner, more complete views along the planned 3D camera paths.
    </figcaption>
  </figure>

  <p>
    They also report quantitative numbers.
    For multi-shooting / multi-trajectory video generation
    (like our “multi-camera rig” example),
    GS-DiT beats those baselines in perceptual quality metrics:
    PSNR (higher is better), SSIM (higher is better),
    LPIPS (lower is better).
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/table3.png"
         alt="Table 3: PSNR, SSIM, LPIPS comparison between GS-DiT and baselines (MonST3R, GCD) on datasets like DAVIS, Sora, Pixabay..." />
    <figcaption>
      Table 3 (Bian et al., 2025).
      GS-DiT outperforms MonST3R and GCD on PSNR / SSIM / LPIPS
      across multiple datasets (DAVIS, Sora-like data, Pixabay, etc.).
      In plain English: its generated new views look more like
      real high-quality footage.
    </figcaption>
  </figure>


  <h2>6. Takeaways and what this means</h2>

  <p>
    The jump from “edit my 2D video” to “control a <b>camera</b> inside my video”
    is huge. GS-DiT treats one normal clip not just as pixels,
    but as evidence about a 3D scene unfolding over time.
    With dense 3D point tracking and a pseudo 4D Gaussian field,
    it can re-shoot that scene with new camera motions,
    do dolly zooms, perform object edits, and even hand you multiple
    “angles” like a film crew.
  </p>

  <p>
    Technically, the key trick is using geometry as a contract.
    Instead of just saying to the diffusion model:
    “Please invent some cool motion,”
    GS-DiT says:
    “Here is the recovered 3D camera path.
    Here is what the scene should look like from that pose,
    rendered from the pseudo 4D Gaussian field.
    Make <i>this</i> look clean and realistic.”
  </p>

  <p>
    Practically, this feels very close to a new workflow for
    filmmaking, VFX, AR/VR, and robotics simulation:
    you shoot a quick phone clip,
    then you generate pro-level camera moves and alternative takes.
    It also hints at future “text → world → controllable camera” pipelines,
    where you could describe a scene in text and immediately get something
    you can direct like a real set.
  </p>


  <footer>
    <h3>References / figure credits</h3>
    <p>
      Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., &amp; Li, H.
      “<i>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields
      through Efficient Dense 3D Point Tracking.</i>”
      arXiv:2501.02690, 2025.
    </p>

    <p>
      Peebles, W., &amp; Xie, S.
      “<i>Scalable Diffusion Models with Transformers.</i>”
      arXiv:2212.09748, 2022.
      (Original DiT architecture.)
    </p>

    <p style="font-size:0.9rem; color:#666;">
      All figures (Figure 1–5, Tables 1–3), demo frames, and composite screenshots
      are from Bian et al. (2025) and the official GS-DiT project page.
      The DiT block diagram is adapted from Peebles &amp; Xie (2022).
      These materials are shown here only for academic review / teaching purposes
      and are not my own generated results.
    </p>
  </footer>

</body>
</html>
