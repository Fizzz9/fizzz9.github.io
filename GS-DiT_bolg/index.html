<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>GS-DiT Blog Post – CAS2105</title>
  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
      font-weight: 600;
    }

    h1 {
      font-size: 2.2rem;
      line-height: 1.25;
      margin-bottom: 0.75rem;
      text-align: left;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img,
    figure video {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #fff;
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    code {
      background: #eee;
      border-radius: 4px;
      padding: 0.15rem 0.4rem;
      font-size: 0.9rem;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    /* meta header under title */
    .meta {
      color: #444;
      font-size: 1rem;
      line-height: 1.5;
      margin-bottom: 2rem;
    }

    .meta-line-strong {
      font-weight: 600;
      color: #111;
    }

    /* reusable wide figure wrapper */
    .wide-figure {
      max-width: 850px;
      margin-left: auto;
      margin-right: auto;
    }

    /* video demo blocks */
    .video-demo-block {
      margin: 2rem 0;
    }

    .video-demo-title {
      font-size: 1.1rem;
      font-weight: 600;
      text-align: center;
      margin-bottom: 1rem;
    }

    .video-row-2,
    .video-row-4 {
      display: grid;
      grid-gap: 1.5rem;
      align-items: start;
      justify-items: center;
    }

    .video-row-2 {
      grid-template-columns: repeat(2, 1fr);
    }

    .video-row-4 {
      grid-template-columns: repeat(4, 1fr);
    }

    .video-col {
      text-align: center;
      max-width: 100%;
    }

    .video-col video {
      width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #000;
    }

    .video-label {
      font-size: 0.85rem;
      color: #555;
      margin-top: 0.5rem;
    }
  </style>
</head>
<body>

  <!-- =============== PAPER TITLE BLOCK =============== -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="../assets/GS-DiT/title.png"
         alt="Paper title: GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      GS-DiT (Bian et al., 2025).  
      Goal: take a normal single video, understand it in 3D + time, then let us shoot new camera motions, edit objects, even get multiple 'virtual cameras' from that one clip.
    </figcaption>
  </figure>

  <!-- =============== BLOG HEADER META =============== -->
  <h1>GS-DiT: Turning a Normal Video into a Controllable 3D Camera Shot</h1>
  <div class="meta">
    <div class="meta-line-strong">Zehao Zhang (Yonsei University)</div>
    <div class="meta-line-strong">Oct 27, 2025</div>
    <div>
      Based on: “GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking,”
      Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, and Hongsheng Li (2025).
    </div>
  </div>

  <!-- =============== SECTION 0 =============== -->
  <h2>0. Why should we care?</h2>

  <p>
    Think about this very real situation: you shoot a short clip with your phone.
    The camera is basically locked — maybe you just hold it in front of your dog at the beach.
    Later you wish: “I want a smooth orbit around the dog,”
    “can I get a dramatic dolly zoom?”, or even
    “give me four angles at once like a movie set.”
    But you didn’t film those shots. You have only <b>one</b> boring clip.
  </p>

  <p>
    GS-DiT says: you <em>can</em> still get those shots.
    From a single video, it can:
  </p>

  <ul>
    <li><b>Move the virtual camera</b> in 3D (pan, tilt, orbit, crane, dolly zoom).</li>
    <li><b>Edit objects</b> in the scene while keeping everything else realistic.</li>
    <li><b>Create multiple camera trajectories</b> for the <i>same</i> moment,
        like you had several cameras filming in parallel.</li>
  </ul>

  <p>
    In other words: “shoot once, direct later.”
  </p>

  <figure class="wide-figure">
    <video controls src="../assets/GS-DiT/corgi.mov"></video>
    <figcaption>
      Project teaser (official GS-DiT demo).  
      One cute corgi-on-the-beach clip becomes many different cinematic views.
      We did not physically fly a drone around the dog.  
      GS-DiT is inventing those camera paths while keeping the dog, towel, and beach consistent.
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <p>
    This is different from “just upscale my video” or “add a filter.”
    GS-DiT tries to <b>understand the scene as 3D over time</b>.
    That lets it generate <i>new</i> viewpoints that feel like real camera motion,
    not just 2D warping.
  </p>

  <!-- =============== SECTION 1 =============== -->
  <h2>1. Quick background: what is a DiT, and why isn’t plain DiT enough?</h2>

  <p>
    DiT stands for <b>Diffusion Transformer</b> — basically a diffusion model
    where the denoising network is a Transformer instead of a U-Net
    (see the “DiT: Scalable Diffusion Models with Transformers” line of work, 2022).
    A vanilla DiT takes a noisy latent (an image or a short video clip in latent space),
    runs a stack of Transformer blocks with attention, and gradually predicts how
    to remove the noise step by step.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/dit.png"
         alt="Typical Diffusion Transformer (DiT) block: tokens from a noised latent go through self-attention, MLP, normalization, conditioning, etc." />
    <figcaption>
      A typical Diffusion Transformer (DiT).  
      You encode frames into latent tokens, add noise, then a Transformer block repeatedly
      denoises those tokens using attention and MLP layers.  
      This works great for generating new images or short videos from prompts,
      but “plain” DiT does not actually know real 3D camera geometry.
      (Diagram based on DiT-style architectures, arXiv:2212.09748)
    </figcaption>
  </figure>

  <p>
    Here’s the problem: a standard DiT mostly sees frames as 2D grids.
    Even if you tell it “tilt the camera down,” it doesn’t truly know where
    “down in 3D space” is.  
    It might just squash/stretch pixels to imitate motion instead of simulating
    a physical camera moving through a 3D scene.
  </p>

  <p>
    GS-DiT’s main contribution is: give the diffusion model <b>actual 3D+time structure</b>
    from the input video, so it can render from <i>new</i> camera poses like a real scene.
  </p>

  <!-- =============== SECTION 2 =============== -->
  <h2>2. Core idea of GS-DiT</h2>

  <p>
    The paper combines two key ingredients:
  </p>

  <ul>
    <li>
      <b>Dense 3D point tracking.</b><br/>
      Instead of just tracking pixels in 2D, GS-DiT tracks a lot of points in <i>3D</i>
      across the whole video. For each little patch of the scene,
      it estimates “this physical point is here in 3D at frame 1, here at frame 2, …”.
      So we start to know the scene layout and how it moves.
    </li>

    <li>
      <b>Pseudo 4D Gaussian field (3D + time).</b><br/>
      Using those tracked 3D points plus estimated depth,
      GS-DiT builds a lightweight scene representation that changes over time.
      You can think of it like a soft 4D “cloud” made of many tiny colored Gaussians:
      X, Y, Z in space, plus t for time.
    </li>
  </ul>

  <p>
    Once we have this pseudo 4D Gaussian field, we can <b>re-render</b> the scene
    from <i>any camera pose we choose</i> — even poses that never existed in the
    original footage.  
    Those new rendered views may be rough (holes, noise), but crucially
    they are aligned to a physically meaningful camera path in 3D.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure1.png"
         alt="Figure 1: overall pipeline. Input video -> dense 3D point tracking -> pseudo 4D Gaussian field -> Gaussian rendering from new views -> final diffused video." />
    <figcaption>
      Fig&nbsp;1 (Paper Fig.1). Pipeline overview.  
      • Left: we start with a normal video and run dense 3D point tracking.<br/>
      • Middle: we lift those tracked points + depth into a pseudo 4D Gaussian field
        (a kind of world model that knows where stuff is in space and time).<br/>
      • Right: we render that field from a <i>new</i> camera path, then ask a diffusion model
        to clean it up into a realistic video.  
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <p>
    This is the big shift: “camera control” stops being a vague text instruction
    and becomes an <b>explicit camera trajectory in 3D</b>.  
    GS-DiT can say, “Here’s the path the virtual camera will fly.
    Here is what that path should see, according to our 4D Gaussian world.”
    Then diffusion turns that rough prediction into photorealistic frames.
  </p>

  <h3>Why tracking quality matters</h3>

  <p>
    If the 3D tracking is wrong, everything collapses:
    geometry tears, backgrounds float, objects smear.
    So the paper benchmarks their tracking system against prior work,
    both in 2D point tracking and in 3D point tracking.
    Better tracking = more stable new views.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/table1ang2.png"
         alt="Table 1 and Table 2: TAPVid and TAPVid-3D tracking scores compared with previous methods." />
    <figcaption>
      Tables&nbsp;1 &amp; 2 (Paper Tables 1 &amp; 2).  
      GS-DiT’s dense point tracking matches or beats strong baselines on datasets like TAPVid,
      TAPVid-3D, DriveTrack, Aria, etc.  
      Metrics like AJ, OA, APD, 3D-AJ measure “are we following the same physical point
      over time, and in 3D?”  
      High-quality tracking is the foundation that lets the model move the camera later
      without everything falling apart.  
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <!-- =============== SECTION 3 =============== -->
  <h2>3. How GS-DiT is trained</h2>

  <p>
    Training-wise, GS-DiT doesn’t just guess out of nowhere.
    It uses two aligned video streams during training:
  </p>

  <ul>
    <li><b>Input video</b>: the original camera view you actually filmed.</li>
    <li><b>Condition video</b>: a <i>novel</i> view rendered from the pseudo 4D Gaussian field,
        following some new camera pose (for example, orbiting left or tilting down).</li>
  </ul>

  <p>
    Both videos get encoded by a 3D VAE encoder into latent tensors.
    Those latents are concatenated and then fed into a Diffusion Transformer.
    Parts of the encoder are frozen, parts are learnable — so the model learns:
    “Given this target camera path, produce clean frames that look like the same scene
    from that new path.”
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure3.png"
         alt="Figure 3: training diagram. Input video latent + condition (rendered novel view) latent -> concatenated -> DiT denoiser." />
    <figcaption>
      Fig&nbsp;2 (Paper Fig.2). Training / conditioning.  
      Top row: the “condition video,” which comes from the new camera pose we want.<br/>
      Bottom row: the original input video.<br/>
      Both are turned into latent features by a 3D VAE encoder.
      The Diffusion Transformer (DiT) then learns to denoise and produce the final output.
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <p>
    Put simply: GS-DiT learns to <b>finish the shot</b>.  
    The 4D Gaussian field gives a rough multi-view guess.
    The diffusion model learns to polish that guess into a stable, pretty video that
    actually follows the requested camera motion.
  </p>

  <h3>Appearance consistency over viewpoints and time</h3>

  <p>
    Another thing GS-DiT cares about: the same object should look like itself,
    even if we rotate around it.  
    For example, if you’re orbiting around an otter surfing on a board,
    the otter shouldn’t suddenly change style frame-to-frame.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure2.png"
         alt="Figure 2: multi-view otter example. The subject keeps identity and style across camera motions." />
    <figcaption>
      Fig&nbsp;3 (Paper Fig.3). Multi-view / multi-time consistency.  
      The same subject (here an otter-like surfing character) is seen from several
      new camera angles. The outfit, style, and pose stay consistent.  
      This shows GS-DiT is not just hallucinating random new objects each time —
      it’s keeping track of the same 3D creature over time.  
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <!-- =============== SECTION 4 =============== -->
  <h2>4. What can GS-DiT actually do?</h2>

  <p>
    Let’s look at concrete abilities from the paper and project page.
    Each demo starts with a boring real clip and ends with something that looks like
    a planned film shot.
  </p>

  <h3>4.1 Camera pose control</h3>

  <p>
    We can literally say:
    “Tilt the camera down,”
    or “move the virtual camera to a new position,”
    and GS-DiT will render what that <i>new</i> camera should see.
    The scene is still the same bubble-on-branch, or the same desk clock —
    just shot from a new angle that never existed in the original footage.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/ec1442e2-2365-434e-93c9-36ded83cf4dd.png"
         alt="Camera pose control: bubble scene and clock scene, input vs output." />
    <figcaption>
      Camera Pose Control (project demo).  
      Top pair: an ice bubble on a frozen branch.<br/>
      Bottom pair: a desk clock.  
      Left = original handheld shot.  
      Right = GS-DiT’s “move/tilt the camera” version.  
      The background and lighting still feel like the same place.
    </figcaption>
  </figure>

  <div class="video-demo-block">
    <div class="video-demo-title">Camera Pose Control – Bubble Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/bubble.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/bubble-1b87f55dec310243.mp4"></video>
        <div class="video-label">Output Video (new virtual camera path)</div>
      </div>
    </div>
  </div>

  <div class="video-demo-block">
    <div class="video-demo-title">Camera Pose Control – Clock Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/clock2.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/clock2-rotate_down_camera_poses_30_10.mp4"></video>
        <div class="video-label">Output Video (tilt / rotate down)</div>
      </div>
    </div>
  </div>

  <h3>4.2 Dolly zoom</h3>

  <p>
    A <b>dolly zoom</b> is a famous film trick: the camera physically moves forward
    while the lens zooms out, so the subject stays roughly the same size,
    but the background stretches in a dramatic way (think Hitchcock / Vertigo shot).
    That normally needs careful camera rigging.
  </p>

  <p>
    GS-DiT can fake that cinematic effect from a plain handheld-looking clip.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/4f3b8d5e-d430-4502-b74e-167d146711d3.png"
         alt="Dolly zoom with camel: left original, right dramatic dolly zoom look." />
    <figcaption>
      Dolly Zoom (project demo).  
      Left row: original camel clip.  
      Right row: GS-DiT output with a dolly zoom style move.
      The camel stays calm and centered, while the background “pushes” and bends
      in that classic thriller-movie way.  
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <div class="video-demo-block">
    <div class="video-demo-title">Dolly Zoom – Camel Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/camel2.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/camel2-id_camera_poses.mp4"></video>
        <div class="video-label">Output Video (dolly zoom style)</div>
      </div>
    </div>
  </div>

  <h3>4.3 Object editing in video</h3>

  <p>
    GS-DiT can also edit an object — for example, change the look of a spinning toy windmill —
    while keeping the rest of the frame, depth, lighting, and “camera feel”
    consistent with reality.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/13027d12-2e1e-44e3-ad1e-0a095e6153ab.png"
         alt="Object editing: windmill blades before and after, with same background and camera." />
    <figcaption>
      Object Editing (project demo).  
      We alter the color / texture / appearance of the little windmill blades.
      The flower garden and bokeh background stay believable.
      This is not frame-by-frame Photoshop; it’s a scene-level edit.
    </figcaption>
  </figure>

  <div class="video-demo-block">
    <div class="video-demo-title">Object Editing – Windmill Scene</div>
    <div class="video-row-2">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/windmill.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/windmill-rotate_down_camera_poses_30_7.mp4"></video>
        <div class="video-label">Output Video (edited object)</div>
      </div>
    </div>
  </div>

  <p>
    The paper groups shots like dolly zoom and object editing together
    under “controllable video generation”: we’re not just passively watching;
    we’re actively directing camera <em>and</em> content.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure5.png"
         alt="Figure 5: Dolly Zoom and Object Editing examples from the paper." />
    <figcaption>
      Fig&nbsp;4 (Paper Fig.5).  
      Left: Dolly Zoom effect.  
      Right: Object Editing.  
      These are creative tools for filmmakers: one changes how the scene is shot,
      the other changes what’s actually in the shot.
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <h3>4.4 Multi-camera shooting from one clip</h3>

  <p>
    Maybe the most “movie studio” result:
    you give GS-DiT one drone-ish coastal clip.
    It then produces <b>several</b> different camera trajectories for that same moment,
    as if you had four cameras in the air at the same time.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/a2a82812-ac10-4a8f-9077-f1492b802f82.png"
         alt="Multi-camera shooting: original coastal clip + 3 new trajectories." />
    <figcaption>
      Multi-camera Shooting (project demo).  
      Left: the original coastline shot.  
      Right: three distinct “virtual cameras”: rotate left, rotate right,
      rotate down-left.  
      You can cut between these angles in editing even though they were never filmed.
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <div class="video-demo-block">
    <div class="video-demo-title">Multi-camera Shooting – Coastal Scene</div>
    <div class="video-row-4">
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur.mp4"></video>
        <div class="video-label">Input Video</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur-rotate_left_camera_poses_30_20.mp4"></video>
        <div class="video-label">Output Trajectory 1</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur-rotate_right_camera_poses_30_20.mp4"></video>
        <div class="video-label">Output Trajectory 2</div>
      </div>
      <div class="video-col">
        <video controls src="../assets/GS-DiT/big_sur-rotate_downleft_camera_poses_30_20.mp4"></video>
        <div class="video-label">Output Trajectory 3</div>
      </div>
    </div>
  </div>

  <!-- =============== SECTION 5 =============== -->
  <h2>5. How does GS-DiT compare to previous methods?</h2>

  <p>
    The paper compares GS-DiT to strong baselines like GCD and MonST3R.
    Older methods often “break” when you ask for a new camera motion:
    backgrounds tear open into black voids, objects smear or get chopped off,
    geometry pops in and out.
  </p>

  <p>
    GS-DiT stays more stable.
    It keeps the subject intact (a swan, a cow, a person, even that surfing otter),
    and it fills in plausible surroundings instead of giant holes.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/figure4.png"
         alt="Figure 4: qualitative comparison across multiple scenes (swan, bike rider, car, cow, portrait, otter). Columns: Input, Camera Poses, Ours, GCD, MonST3R." />
    <figcaption>
      Fig&nbsp;5 (Paper Fig.4). Qualitative comparison.  
      Columns (left→right): the original input video, the recovered camera poses,
      GS-DiT’s output, and two baselines (GCD, MonST3R).  
      Rows: swan on water, bike rider, driving car, cow in a field,
      portrait of an old man, the surfing otter.  
      GS-DiT produces cleaner, more complete novel views with fewer black gaps.
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <p>
    They also report quantitative metrics for multi-camera / multi-trajectory generation:
    PSNR and SSIM should go <b>up</b> (higher = closer to ground truth),
    LPIPS should go <b>down</b> (lower = perceptually closer).
    GS-DiT wins on these across datasets like DAVIS, Sora, and Pixabay.
  </p>

  <figure class="wide-figure">
    <img src="../assets/GS-DiT/table3.png"
         alt="Table 3: PSNR, SSIM, LPIPS comparisons for multi-shooting video generation quality." />
    <figcaption>
      Table&nbsp;3 (Paper Table 3).  
      GS-DiT beats GCD and MonST3R in PSNR↑ / SSIM↑ / LPIPS↓ for generated novel views.
      Translation: the new camera shots are not only cooler — they’re measurably closer
      to what a real camera at that angle should have seen.  
      (Bian et al., 2025)
    </figcaption>
  </figure>

  <!-- =============== SECTION 6 =============== -->
  <h2>6. The pipeline in plain steps</h2>

  <p>Here is GS-DiT as a recipe:</p>

  <ol style="padding-left:1.25rem;">
    <li>
      <b>Start with a normal video.</b><br/>
      Track dense points in 3D across time. Estimate depth.  
      Now the model has a rough idea of scene geometry and motion.
    </li>

    <li>
      <b>Build a pseudo 4D Gaussian field.</b><br/>
      This is the “world model”: a time-varying set of colored 3D Gaussians
      that says “what is where” at each frame.
    </li>

    <li>
      <b>Pick any new camera path.</b><br/>
      Orbit around the subject, tilt down, do a dolly zoom, etc.
      This path is full 6-DoF (3D location + 3D rotation over time),
      just like a real camera rig.
    </li>

    <li>
      <b>Render that path from the 4D field.</b><br/>
      The render may look incomplete (holes, noise),
      but it’s physically aligned to your requested camera motion.
    </li>

    <li>
      <b>Refine with a Diffusion Transformer.</b><br/>
      A DiT denoiser turns that rough render into a clean, stable,
      photorealistic video clip.
    </li>

    <li>
      <b>Repeat for more paths or edits.</b><br/>
      You can now generate multiple “virtual takes” of the same moment,
      or edit specific objects, without reshooting anything.
    </li>
  </ol>

  <p>
    So GS-DiT is basically an automatic mini film crew:
    it understands your scene in 3D + time,
    it lets you direct the camera afterward,
    and it outputs shots that look like you actually filmed them.
  </p>

  <!-- =============== SECTION 7 =============== -->
  <h2>7. Why this matters (my take)</h2>

  <p>
    This work is one step toward “text-to-world,” but grounded in real footage.
    We’re moving from “please generate a pretty clip” to
    “please give me this exact shot.”
  </p>

  <p>
    For creators and video editors:
    you can shoot something fast — even handheld —
    and later ask for pro-looking moves (orbit, dolly zoom, crane),
    or clean object edits, without renting gear or doing reshoots.
  </p>

  <p>
    For AR / VR / robotics:
    being able to reconstruct a controllable 3D+time scene from a short casual clip
    is huge. It means you can drop a virtual camera anywhere,
    not just replay the original path.
  </p>

  <p>
    There are still limits:  
    if the video never shows the back of an object, GS-DiT has to hallucinate it.
    And diffusion is still expensive to run.  
    But the direction is very clear:
    we’re leaving the world of “I filmed this clip.”
    We’re entering “I filmed some raw material, and now I can direct it however I want.”
  </p>

  <!-- =============== REFERENCES =============== -->
  <footer>
    <h3>References / Figure credits</h3>
    <p>
      Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., and Li, H.
      “<i>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking.</i>”
      arXiv:2501.02690, 2025.
    </p>
    <p>
      Peebles, W., and Xie, S.
      “<i>Scalable Diffusion Models with Transformers.</i>”
      arXiv:2212.09748, 2022.
      (This is the Diffusion Transformer / DiT background model.)
    </p>
    <p>
      All figures (Fig.1–5), tables (Tables 1–3), and demo videos
      are adapted from Bian et al. (2025) and the official GS-DiT project page.
      They illustrate: dense 3D point tracking and pseudo 4D Gaussian fields (Fig.1),
      training with input+condition views and DiT denoising (Fig.2),
      appearance consistency across viewpoints (Fig.3),
      qualitative comparison vs. GCD / MonST3R (Fig.4),
      controllable shots such as dolly zoom, camera pose control, object editing,
      and multi-camera shooting (Fig.5), plus tracking / quality metrics (Tables 1–3).
    </p>
  </footer>

</body>
</html>
