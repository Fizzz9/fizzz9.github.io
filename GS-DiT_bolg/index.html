<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>GS-DiT Blog Post – CAS2105</title>

  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
      font-weight: 600;
    }

    h1 {
      font-size: 2.3rem;
      line-height: 1.22;
      margin-bottom: 0.75rem;
      text-align: left;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #fff;
    }

    figure video {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #000;
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    .meta {
      color: #444;
      font-size: 1rem;
      line-height: 1.5;
      margin-bottom: 2rem;
    }

    .meta-line-strong {
      font-weight: 600;
      color: #111;
    }

    .wide-figure {
      max-width: 850px;
      margin-left: auto;
      margin-right: auto;
    }
  </style>
</head>
<body>

  <!-- Paper title figure -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="assets/GS-DiT/title.png"
         alt="Paper title: GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking (Bian et al., 2025)" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      GS-DiT (Bian et al., 2025). A controllable video generation system built on dense 3D point tracking and a pseudo 4D Gaussian field.
    </figcaption>
  </figure>

  <!-- Blog title / meta -->
  <h1>GS-DiT: Turning a Normal Video into a Controllable 3D Camera Shot</h1>
  <div class="meta">
    <div class="meta-line-strong">Zehao Zhang (Yonsei University)</div>
    <div class="meta-line-strong">Oct 27, 2025</div>
    <div>
      Based on: “GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking,”
      Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, and Hongsheng Li (2025).
      Also referencing Peebles &amp; Xie, “Scalable Diffusion Models with Transformers” (2022) for the base DiT architecture.
    </div>
  </div>

  <!-- hero demo video -->
  <figure class="wide-figure">
    <video src="assets/GS-DiT/corgi.mov"
           autoplay
           loop
           muted
           playsinline
           controls
           style="max-width:100%; height:auto;">
    </video>
    <figcaption>
      Demo from the official GS-DiT project page.
      One casual beach clip of a corgi with a GoPro becomes
      multiple smooth “virtual camera views,” like you had several cameras on set.
      This is the core promise of GS-DiT.
    </figcaption>
  </figure>

  <!-- ======================== SECTION 0 ======================== -->
  <h2>0. Why should we care about this paper?</h2>

  <p>
    Imagine you film something simple with your phone: you point at your friend / pet / object,
    hold still, maybe move a tiny bit. Now pretend you’re a director and you say:
    “Can you give me an orbit shot?” “Can we get a dolly zoom?” “Can I have three camera angles
    of the exact same moment so I can edit like a real movie?”
    In real life, you didn’t shoot any of that. You only have one boring clip.
  </p>

  <p>
    GS-DiT basically says: <b>we can still make those shots</b>.
    It can:
  </p>
  <ul>
    <li>move a <b>virtual camera</b> around the scene in 3D (6-DoF motion) even if you never filmed that motion,</li>
    <li>apply cinematography tricks like <b>dolly zoom</b>,</li>
    <li><b>edit objects</b> inside the video (change their look),</li>
    <li>and even create <b>multiple camera views</b> from one clip,
        like you had a full multi-camera rig.</li>
  </ul>

  <p>
    The key mental shift: GS-DiT does not treat a video as “just 2D pixels over time.”
    It tries to recover a 3D understanding of the scene as it moves in time
    (often called a “4D” view: 3D + time), and then it uses that understanding
    to render new camera views and guide a video diffusion model to fill in details.
    That’s why the output feels like a real shot — not just a wobbly warp.
  </p>


  <!-- ======================== SECTION 1 ======================== -->
  <h2>1. Quick background: what is DiT?</h2>

  <p>
    Diffusion models generate images or videos by starting from random noise
    and then denoising step by step. A lot of famous models (Stable Diffusion, etc.)
    use U-Net backbones for that denoising network.
  </p>

  <p>
    <b>DiT</b> (“Diffusion Transformer”) is a different style:
    instead of a U-Net, it uses a pure Transformer to do the denoising
    (Peebles &amp; Xie, 2022).
    The idea is:
  </p>

  <ul>
    <li>Take the current noisy latent features of the video (or image).</li>
    <li>Split them into tokens (like patches) and feed them to a Transformer.</li>
    <li>Use self-attention + MLP layers to predict how to nudge those latents
        a little closer to the clean result.</li>
    </ul>

  <p>
    Conditioning (like the timestamp in the diffusion process,
    a text prompt, or some control signal) is injected
    through techniques like adaptive LayerNorm (often called adaLN-Zero).
    That lets DiT follow instructions like “make it look like X”.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/dit.png"
         alt="Classic DiT block design with adaLN-Zero and conditioning. Adapted from Peebles & Xie (2022)." />
    <figcaption>
      Classic DiT-style Transformer denoiser with adaptive layer norm (“adaLN-Zero”)
      to inject conditioning (Peebles &amp; Xie, 2022).
      GS-DiT keeps this Transformer-based diffusion core,
      but it adds strong 3D/4D scene supervision and camera control on top.
    </figcaption>
  </figure>

  <p>
    In other words, vanilla DiT = “Transformer that denoises latent video.”
    GS-DiT = “that Transformer, plus a real sense of 3D camera motion.”
  </p>


  <!-- ======================== SECTION 2 ======================== -->
  <h2>2. So what is GS-DiT doing on top of DiT?</h2>

  <p>
    GS-DiT’s main idea is: before you ask a diffusion model to “imagine a new camera move,”
    first give it a rough 3D version of the scene from that new camera pose.
    That 3D guess acts like a contract:
    “Here’s where the camera will be. Here’s what the world should roughly look like
    from that angle. Now please make it realistic.”
  </p>

  <p>
    To do that, GS-DiT builds what the paper calls a
    <b>pseudo 4D Gaussian field</b>.
    Let’s unpack that name:
  </p>

  <ul>
    <li><b>Gaussian field:</b>
      Represent the scene as lots of 3D Gaussian blobs with color, size, orientation.
      This is similar in spirit to recent Gaussian Splatting ideas in 3D vision.
    </li>
    <li><b>4D:</b>
      It’s not static. The scene changes over time (the 4th “dimension”), so those blobs
      also move / update over frames.
    </li>
    <li><b>Pseudo:</b>
      It’s not a perfect full physical reconstruction. It’s an approximate, learned
      field that is “good enough” to render convincing new viewpoints.
    </li>
  </ul>

  <p>
    Once they have this pseudo 4D Gaussian field, they can render
    what the scene would look like
    from any <b>new camera pose / trajectory</b>,
    even if you never filmed from there.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure2.png"
         alt="High-level pipeline: dense 3D point tracking, pseudo 4D Gaussian field, novel-view rendering, and conditioning of DiT." />
    <figcaption>
      Figure (Bian et al., 2025). Pipeline overview.
      Input video → dense 3D point tracking + depth estimation →
      pseudo 4D Gaussian field that can be rendered from new viewpoints.
      Those rendered views condition the diffusion Transformer,
      which produces clean, temporally consistent output.
    </figcaption>
  </figure>

  <p>
    Training time looks like this:
    they take both the original clip and a “condition clip”
    (for example, a novel camera view rendered from their pseudo 4D field,
    or an edited-object version).
    Both clips are encoded with a 3D VAE into latent features.
    Those features are then fed to the DiT backbone,
    which learns to generate the desired target video.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure3.png"
         alt="Training / conditioning setup: encode input video and condition video with a 3D VAE, then guide DiT to generate the target output." />
    <figcaption>
      Figure (Bian et al., 2025). Training / conditioning design.
      The “Condition Video” branch can represent a new camera motion or an edit.
      Both condition and input are encoded by a (frozen) 3D VAE.
      Their latents are concatenated and passed to the DiT denoiser,
      which learns to synthesize the final controlled/edited video.
    </figcaption>
  </figure>

  <p>
    Short version:
    <b>GS-DiT does not just hallucinate cool motion.</b>
    It first guesses geometry + camera pose,
    renders a draft of that motion,
    and only then asks the diffusion Transformer to polish it.
    That’s why the camera obeys you.
  </p>


  <!-- ======================== SECTION 3 ======================== -->
  <h2>3. How does GS-DiT even get 3D from a single handheld video?</h2>

  <p>
    Classic 3D reconstruction (like NeRF, Gaussian Splatting, etc.)
    usually assumes you walked around the object and filmed it from many angles.
    GS-DiT is more aggressive: it tries to pull 3D information
    from just your one clip.
  </p>

  <p>
    The trick is <b>dense 3D point tracking</b>.
    Instead of tracking just a few keypoints, it tries to follow
    a dense set of points all over the scene across frames,
    and estimate their depth and motion in 3D.
    If you can do that well, you basically recover:
  </p>

  <ul>
    <li>which way the camera is moving (its pose over time),</li>
    <li>rough surfaces / layout of the scene (so you know what’s in front/behind),</li>
    <li>where each piece of the scene goes as time passes.</li>
  </ul>

  <p>
    This is not trivial. If the tracking is bad,
    any “virtual camera move” you try to synthesize will tear, smear,
    or show big black holes.
    So the authors benchmark their tracking module
    against strong trackers like TAPIR, CoTracker, SpatialTracker, etc.,
    on TAPVid and TAPVid-3D.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/table1and2.png"
         alt="Table 1 and Table 2: comparison of dense 2D and 3D point tracking performance vs prior methods." />
    <figcaption>
      Tables 1 &amp; 2 (Bian et al., 2025). Dense point tracking quality.
      Their pipeline (“Ours + …”) is competitive or better
      in both 2D (TAPVid) and 3D (TAPVid-3D) tracking metrics.
      Stronger tracking → more reliable pseudo 4D field → more believable camera control.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 4 ======================== -->
  <h2>4. What can GS-DiT actually do with just one clip?</h2>

  <p>
    This is the part that feels like cheating in a good way.
    We’ll walk through four abilities the paper/demo highlights.
    All of the following samples come from Bian et al. (2025)
    and the official GS-DiT project page.
    We’re showing still frames here, but they correspond to generated videos.
  </p>

  <h3>4.1 Camera pose control</h3>

  <p>
    You can literally tell the model:
    “Tilt the camera down a bit,” or
    “Move to a slightly different viewpoint,”
    and GS-DiT will synthesize what that would look like.
    The important part: the scene is still the <b>same</b> scene.
    It doesn’t suddenly turn into a different background.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/cam_pose_control.png"
         alt="Camera pose control examples: bubble scene and clock scene, input vs output." />
    <figcaption>
      Camera Pose Control.
      Top row: a frozen soap bubble on a winter branch.
      Bottom row: a clock on some books.
      Left = original input video.
      Right = GS-DiT output with a controlled camera tilt / move.
      It looks like you re-shot the scene from a new angle.
    </figcaption>
  </figure>

  <h3>4.2 Dolly zoom (cinema trick)</h3>

  <p>
    A “dolly zoom” is a famous movie effect:
    the camera physically moves toward the subject
    while zooming out the lens, so the subject stays about the same size
    but the background stretches dramatically.
  </p>

  <p>
    GS-DiT can create that effect from a normal clip of (for example) a camel.
    You never did a professional dolly move,
    but now you get that Hollywood feel.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/dolly_zoom.png"
         alt="Dolly zoom demo with a camel: input vs GS-DiT output." />
    <figcaption>
      Dolly Zoom.
      Left row: the plain handheld-looking camel clip.
      Right row: GS-DiT’s output with a dramatic dolly-zoom style motion.
      The camel stays stable while the background appears to “push.”
    </figcaption>
  </figure>

  <h3>4.3 Object editing in video</h3>

  <p>
    GS-DiT can also edit an object, frame by frame, while keeping
    the rest of the scene (camera motion, depth cues, lighting) stable.
    In the demo, they change the look of a spinning windmill toy
    in front of flowers.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/obj_edit.png"
         alt="Object editing: windmill blades appearance changed between input and output." />
    <figcaption>
      Object Editing.
      Left: input video with the original toy windmill.
      Right: output where the appearance of the windmill blades is changed.
      Background and camera motion remain realistic.
    </figcaption>
  </figure>

  <h3>4.4 Multi-camera shooting from one clip</h3>

  <p>
    This is maybe the most “film studio” style result.
    Give GS-DiT one drone-like coastal clip.
    It will generate several <b>different camera trajectories</b>:
    rotate left, rotate right, tilt down-left, etc.
  </p>

  <p>
    Now you basically have four camera angles
    of the exact same moment.
    You can cut between them in editing
    like a real multi-camera shoot — even though
    you only recorded once.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/multi_cam.png"
         alt="Multi-camera shooting demo: one coastal clip, plus three synthesized trajectories." />
    <figcaption>
      Multi-camera Shooting.
      Far left: original coastal clip.
      Three to the right: GS-DiT’s synthetic “rotate left,” “rotate right,”
      and “rotate down-left” camera motions.
      Instant multi-cam coverage from a single take.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 5 ======================== -->
  <h2>5. How does GS-DiT compare to previous work?</h2>

  <p>
    Before GS-DiT, there were already single-view novel view synthesis / 4D-ish systems,
    like MonST3R and GCD. They try to take one monocular video
    and produce new viewpoints. The problem is:
    when the camera motion gets more extreme,
    those systems often break — objects stretch, backgrounds tear,
    or you get black holes where nobody knows what should exist.
  </p>

  <p>
    GS-DiT improves stability by doing two things at once:
  </p>
  <ul>
    <li><b>Better geometry estimation</b> via dense 3D point tracking,
        so it actually knows where stuff is in space.</li>
    <li><b>Camera-aware conditioning</b>:
        it renders the pseudo 4D Gaussian field from the <i>exact</i>
        planned virtual camera pose and then conditions the diffusion model on that.
        The model is pushed to honor that camera path instead of drifting.</li>
  </ul>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure4.png"
         alt="Qualitative comparison: Input Video, Camera Poses, Ours, GCD, MonST3R across several scenes like swan, cyclist, car, cow..." />
    <figcaption>
      Figure 4 (Bian et al., 2025). Qualitative comparison.
      Columns: Input, recovered Camera Poses, GS-DiT (“Ours”),
      and two baselines (GCD, MonST3R).
      Rows: swan on water, cyclist by graffiti, cars, cow, human portrait, etc.
      Baselines often produce tearing or big missing chunks.
      GS-DiT keeps the scene more complete and consistent along the novel path.
    </figcaption>
  </figure>

  <p>
    They also report numbers.
    In Table 3 (PSNR↑, SSIM↑, LPIPS↓),
    GS-DiT beats MonST3R and GCD on multi-trajectory video generation quality.
    In plain English: not only does it look nicer,
    it’s objectively closer to real footage by standard vision metrics.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/table3.png"
         alt="Table 3: PSNR, SSIM, LPIPS comparison between GS-DiT and baselines (MonST3R, GCD)." />
    <figcaption>
      Table 3 (Bian et al., 2025).
      On datasets like DAVIS, Sora-like sets, and Pixabay-style clips,
      GS-DiT achieves higher PSNR / SSIM and lower LPIPS
      than MonST3R and GCD.
      So the generated new views are both sharper and more faithful.
    </figcaption>
  </figure>


  <!-- ======================== SECTION 6 ======================== -->
  <h2>6. Takeaways: why this matters</h2>

  <p>
    The big idea of GS-DiT is to treat a normal phone video
    not as “a clip you’re stuck with,”
    but as “evidence of a 3D world you can now direct.”
    That’s a huge change in how we think about video editing.
  </p>

  <p>
    From a technical angle, the clever part is the contract between geometry and generation.
    GS-DiT does <b>not</b> just say to a diffusion model:
    “Please invent something pretty.”
    It says:
    “Here is a dense 3D point track. Here is an approximate 4D Gaussian field.
    Here is the exact 6-DoF camera path we want.
    Render that draft. Now diffusion, clean it up.”
    That is why the camera actually follows instructions
    and why the scene stays coherent.
  </p>

  <p>
    From an application angle, this feels very close to
    “AI cinematography from your phone.”
    You shoot one quick clip,
    and later you can:
    <b>tilt the camera, orbit, dolly zoom, edit an object,
    or cut between three angles of the same moment.</b>
    That’s useful for creative work (short films, ads),
    VR / AR content, even robotics simulation
    where you want to see the same moment from different views.
  </p>

  <p>
    The limits are still real: if the scene is totally unseen
    (e.g. the back side of an object the camera never saw),
    GS-DiT has to hallucinate. And training / inference is expensive,
    because it’s a video diffusion model with 3D tracking in the loop.
    But directionally, this is very close to
    “write a prompt → get a scene → move a camera through it
    like a real director.” We’re not fully there yet,
    but GS-DiT is a big step toward that workflow.
  </p>


  <footer>
    <h3>References / figure credits</h3>
    <p>
      Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., &amp; Li, H.
      “<i>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields
      through Efficient Dense 3D Point Tracking.</i>”
      arXiv:2501.02690, 2025.
    </p>

    <p>
      Peebles, W., &amp; Xie, S.
      “<i>Scalable Diffusion Models with Transformers.</i>”
      arXiv:2212.09748, 2022.
      (Original DiT architecture.)
    </p>

    <p style="font-size:0.9rem; color:#666;">
      All figures (Figure 1–4, Tables 1–3),
      and all demo frames / video snippets (camera pose control, dolly zoom,
      object editing, multi-camera shooting, corgi beach demo)
      are from Bian et al. (2025) and the official GS-DiT project page.
      The DiT block diagram is adapted from Peebles &amp; Xie (2022).
      Shown here only for academic explanation in CAS2105.
    </p>
  </footer>

</body>
</html>
