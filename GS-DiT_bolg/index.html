<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>GS-DiT Blog – CAS2105</title>
  <style>
    body {
      max-width: 900px;
      margin: 2rem auto 5rem;
      padding: 0 1rem;
      font-family: -apple-system, BlinkMacSystemFont, "Inter", "Helvetica Neue", Arial, sans-serif;
      line-height: 1.55;
      color: #1a1a1a;
      background-color: #ffffff;
    }

    h1, h2, h3 {
      line-height: 1.3;
      color: #111;
      font-weight: 600;
    }

    h1 {
      font-size: 2.2rem;
      line-height: 1.22;
      max-width: 75ch;
      margin-bottom: 0.75rem;
      text-align: left;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      border-left: 4px solid #4f46e5;
      padding-left: 0.5rem;
      background: linear-gradient(to right, rgba(79,70,229,0.08), transparent);
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }

    p {
      font-size: 1rem;
      margin: 0.5rem 0 1rem;
    }

    ul {
      padding-left: 1.25rem;
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img,
    figure video {
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #fff;
    }

    figcaption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }

    code {
      background: #eee;
      border-radius: 4px;
      padding: 0.15rem 0.4rem;
      font-size: 0.9rem;
    }

    footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      line-height: 1.4;
      color: #444;
      border-top: 1px solid #ddd;
      padding-top: 1rem;
    }

    .meta {
      color: #444;
      font-size: 1rem;
      line-height: 1.5;
      margin-bottom: 2rem;
    }

    .meta-line-strong {
      font-weight: 600;
      color: #111;
    }

    .wide-figure {
      max-width: 850px;
      margin-left: auto;
      margin-right: auto;
    }

    .hero-video-wrapper {
      max-width: 850px;
      margin: 2rem auto 2rem;
      text-align: center;
    }

    .hero-video-wrapper video {
      width: 100%;
      border-radius: 10px;
      border: 1px solid #ccc;
      background: #000;
      display: block;
    }

    /* small caption under the hero video */
    .hero-caption {
      font-size: 0.9rem;
      color: #555;
      margin-top: 0.5rem;
      line-height: 1.4;
      text-align: center;
    }
  </style>
</head>
<body>

  <!-- Paper title figure -->
  <figure style="text-align:center; margin: 0 0 2rem 0;">
    <img src="assets/GS-DiT/title.png"
         alt="Paper title: GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking" />
    <figcaption style="font-size:0.9rem; color:#666; margin-top:0.5rem;">
      GS-DiT (Bian et al., 2025). Video generation with controllable camera motion,
      built on dense 3D point tracking and pseudo 4D Gaussian fields.
    </figcaption>
  </figure>

  <!-- Blog title / meta -->
  <h1>GS-DiT: Turning a Normal Video into a Controllable 3D Camera Shot</h1>

  <div class="meta">
    <div class="meta-line-strong">Zehao Zhang (Yonsei University)</div>
    <div class="meta-line-strong">Oct 27, 2025</div>
    <div>
      Based on: “GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking,”
      Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, and Hongsheng Li (2025).
    </div>
  </div>

  <!-- Hero teaser video (corgi demo from project page) -->
  <div class="hero-video-wrapper">
    <video src="assets/GS-DiT/corgi.mov"
           autoplay
           muted
           loop
           playsinline>
      Your browser does not support the video tag.
    </video>
    <div class="hero-caption">
      GS-DiT can treat one normal handheld video like raw material for a “virtual camera rig.”
      Above: the same dog scene, but filmed from multiple new viewpoints that were never actually shot.
      (Project demo, Bian et al., 2025.)
    </div>
  </div>

  <!-- 0. Motivation -->
  <h2>0. Why do we care about this paper?</h2>

  <p>
    Imagine you recorded a short clip on your phone. The camera just stands still.
    Later you wish you had a cinematic version: a smooth dolly zoom, a side crane shot,
    or a drone-like orbit circling the subject. But you never shot those moves.
    You only have that one boring clip.
  </p>

  <p>
    GS-DiT says: we can <b>generate</b> those missing camera motions.
    We can also keep objects consistent while editing them (e.g. change colors, shapes),
    and even create multiple alternative “virtual takes” from the same moment —
    like shooting with four different cameras at once.
  </p>

  <p>
    The key idea is that GS-DiT does <b>not</b> just warp pixels in 2D.
    It tries to understand how the scene sits in 3D (and even over time, i.e. “4D”),
    using <b>dense 3D point tracking</b> and a <b>pseudo 4D Gaussian field</b>.
    Once it has that, it can move a <b>virtual</b> camera through the scene and render how
    the world should look from that new trajectory — instead of just faking a zoom.
  </p>

  <!-- We restate idea with first figure -->
  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure1.png"
         alt="Figure 1 from the paper: same otter surfing scene, shown with several different novel camera trajectories" />
    <figcaption>
      Figure 1 (Bian et al., 2025). One short input clip of an otter on a surfboard.
      GS-DiT estimates the 3D motion of the scene,
      then synthesizes brand new camera paths (rows),
      producing different “shots” while keeping the world and the otter consistent.
      This is the core promise: camera <b>control</b> from a single ordinary video.
    </figcaption>
  </figure>


  <!-- 1. A quick refresher: what is DiT and why does this paper build on it? -->
  <h2>1. Background: DiT and controllable video diffusion</h2>

  <p>
    Modern image/video generation is often done with diffusion models:
    you start from noise and iteratively denoise toward a realistic sample.
    A diffusion model needs a backbone (the “brain” that predicts how to denoise).
    <b>DiT</b> (Diffusion Transformer) is a strong backbone architecture that uses a pure Transformer,
    not a U-Net, to run diffusion in latent space. It first appeared in
    “Scalable Diffusion Models with Transformers” (Peebles &amp; Xie, 2022, arXiv:2212.09748),
    and it showed that Transformers can scale very well for high-quality image synthesis.
  </p>

  <p>
    Very roughly, a vanilla DiT block looks like this:
    patches (tokens) of the latent are fed through self-attention and MLP layers,
    and the network is conditioned on things like timestep and text prompt.
    The figure below sketches a generic DiT-style latent diffusion transformer block:
    tokens go through LayerNorm, multi-head self-attention, feedforward layers,
    and conditioning (scale/shift) is injected at each block.
    This is the standard “denoising brain.”
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/dit.png"
         alt="High-level structure of a DiT block with conditioning, based on Peebles & Xie 2022" />
    <figcaption>
      Figure (conceptual DiT block). Standard DiT: a latent diffusion Transformer.
      Each block applies self-attention and feedforward layers to latent tokens,
      with learned conditioning (like timestep / guidance) injected by scale-and-shift.
      This comes from the DiT line of work (Peebles &amp; Xie, 2022, arXiv:2212.09748),
      and is the backbone that GS-DiT extends to video + camera control.
    </figcaption>
  </figure>

  <p>
    So DiT already gives us a powerful Transformer-based diffusion engine.
    But “plain DiT” doesn’t know anything about:
  </p>

  <ul>
    <li>where the camera is in 3D,</li>
    <li>how scene points move in 3D over time,</li>
    <li>how to generate consistent <i>novel</i> views of the same moment.</li>
  </ul>

  <p>
    GS-DiT takes that DiT backbone and bolts on two big ideas:
  </p>

  <ul>
    <li>
      <b>Dense 3D point tracking</b>: follow many points in the scene in 3D,
      across frames, so we know “this exact twig / pixel / fur spot moved like this in space.”
    </li>
    <li>
      <b>Pseudo 4D Gaussian field</b>: turn that tracked 3D information into something like
      a lightweight 4D representation (3D over time) that we can render from new viewpoints.
      This is “pseudo 4D” because it’s not a full physical reconstruction, but it’s good enough
      to supervise controllable novel views.
    </li>
  </ul>

  <p>
    With those two, GS-DiT can tell its diffusion model:
    “Here is what the scene looks like from a <i>new camera pose</i> that we never actually filmed.
    Please generate frames consistent with that pose.”
  </p>


  <!-- 2. Core pipeline -->
  <h2>2. How does GS-DiT actually work?</h2>

  <p>
    At a high level, the pipeline looks like this:
    (1) track dense 3D points through the video,
    (2) lift them into a pseudo 4D Gaussian field,
    (3) render that field from a new, user-chosen camera trajectory,
    (4) feed those renders as conditioning to a DiT-style video diffusion model,
    which produces clean, high-quality frames for the new shot.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure2.png"
         alt="Figure 2 from the paper: overview of GS-DiT system, showing dense 3D tracking, pseudo 4D Gaussian field, and camera control" />
    <figcaption>
      Figure 2 (Bian et al., 2025). System overview.
      Left: we start with an input video and run dense 3D point tracking +
      depth lifting, recovering how the scene moves in space.
      Middle: we build a pseudo 4D Gaussian field — basically a set of Gaussian splats
      in 3D, evolving over time — that can be rendered from arbitrary camera views.
      Right: those rendered views become “condition videos” that guide the diffusion model,
      so GS-DiT can generate novel-view or controlled-view output clips.
    </figcaption>
  </figure>

  <h3>2.1 Dense 3D point tracking</h3>
  <p>
    The paper’s first technical pillar is <b>dense 3D point tracking</b>.
    For every visible pixel patch, GS-DiT tries to follow that same physical point
    across frames, and also infer its depth.
    This is harder than normal optical flow (2D motion),
    because it tries to stay consistent in <b>3D space</b>, not just the image plane.
  </p>

  <p>
    Why is that useful?  
    If I know “this specific corner of the panda’s ear moved like <i>this</i> in 3D,”
    then I can guess what that ear would look like if my camera moved to the left,
    even if my phone never actually went left.
    This is what lets GS-DiT do camera control from monocular footage.
  </p>

  <h3>2.2 Pseudo 4D Gaussian field</h3>
  <p>
    From those tracked points, GS-DiT builds a <b>pseudo 4D Gaussian field</b>:
    think “a bunch of 3D Gaussian blobs that also evolve over time.”
    Each Gaussian stores color/appearance and where it lives in space (and time).
    It’s “4D” because we model not just XYZ in one frame, but XYZ across the video timeline.
  </p>

  <p>
    Why Gaussians?  
    Rendering point-based Gaussian splats has become popular in neural rendering
    (it’s fast and differentiable). Here, the pseudo 4D field acts like a lightweight
    scene proxy. We can <b>render</b> it from any camera pose we want:
    rotate left, tilt down, dolly forward, etc.
  </p>

  <h3>2.3 Conditioning a DiT-based video diffusion model</h3>
  <p>
    The last stage: all those “virtual camera renders” are fed into a Transformer-based
    diffusion model (a DiT variant) that actually synthesizes the final frames.
    Instead of starting from pure noise, the model is guided by both:
  </p>

  <ul>
    <li>the original input clip (what the scene really looked like), and</li>
    <li>the rendered novel-view clip from the pseudo 4D field (what we <i>want</i> the new camera to see).</li>
  </ul>

  <p>
    The model fuses them and denoises into a clean, realistic output video.
    So the diffusion model is not hallucinating blindly — it is <b>conditioned</b>
    on explicit geometry and camera pose.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure3.png"
         alt="Figure 3 from the paper: conditioning scheme where input video and condition video go through a 3D VAE, then feed a DiT" />
    <figcaption>
      Figure 3 (Bian et al., 2025). Conditioning.
      Both the “condition video” (rendered from the pseudo 4D field at the target camera pose)
      and the original input video are encoded by a 3D VAE.
      Their latent features are concatenated and given to a DiT-style diffusion transformer,
      which predicts noise and denoises toward the final novel-view frames.
      Frozen parts vs. trainable parts are also indicated.
    </figcaption>
  </figure>


  <!-- 3. What do we get out of it -->
  <h2>3. What do we get out of it?</h2>

  <p>
    Once GS-DiT understands the scene as a pseudo 4D Gaussian field
    and learns to render/denoise from arbitrary camera poses,
    we unlock several powerful abilities:
  </p>

  <ul>
    <li><b>Camera pose control</b>: “move” the camera even if you never filmed that move.</li>
    <li><b>Dolly zoom / cinematic shots</b>: create film-style zooms and push-ins that weren’t in the raw video.</li>
    <li><b>Object editing</b>: change the appearance of a specific object over time,
        while keeping the rest of the clip stable.</li>
    <li><b>Multi-camera shooting</b>: generate multiple different camera trajectories
        from the same moment, like you had several extra cameras on set.</li>
  </ul>


  <!-- 4. Demos -->
  <h2>4. Visual demos of GS-DiT’s abilities</h2>

  <h3>4.1 Camera pose control</h3>
  <p>
    You can literally say “tilt the camera down a bit,” or “swing to a new viewpoint,”
    and GS-DiT will synthesize new frames that match that new virtual camera.
    The content still looks like the same real scene.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/camera_pose_control.png"
         alt="Camera pose control examples: bubble scene and clock scene, input vs output" />
    <figcaption>
      Camera Pose Control (project demo).
      Top row: a frozen soap bubble on an icy branch.
      Bottom row: a desk clock.
      Left = original handheld video.
      Right = GS-DiT output with a controlled camera motion (tilt / orbit) that did not exist in the input.
      The background and lighting stay believable.
    </figcaption>
  </figure>

  <h3>4.2 Dolly zoom</h3>
  <p>
    A “dolly zoom” is a famous movie trick: the camera moves forward while zooming out,
    so the subject size stays almost the same but the background stretches.
    GS-DiT can create this effect from a plain clip that never actually did that move.
    Under the hood, GS-DiT is generating a <b>new</b> virtual camera trajectory,
    rendering it via the pseudo 4D Gaussian field, then denoising via DiT.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/dolly_zoom.png"
         alt="Dolly zoom demo with camel: input vs output" />
    <figcaption>
      Dolly Zoom (project demo).
      Left row: original video of a camel, just a normal handheld shot.
      Right row: GS-DiT output with a dramatic dolly zoom feel —
      background “pushes” while the camel remains nicely framed.
    </figcaption>
  </figure>

  <h3>4.3 Object editing in video</h3>
  <p>
    GS-DiT can edit a specific object but keep the rest of the clip stable.
    For example, we can change the color / material style of a spinning windmill toy
    while preserving the background, depth, and camera motion.
    The model basically re-renders that region with new appearance, still respecting
    the tracked 3D geometry.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/object_editing.png"
         alt="Object editing: windmill blades changed between input and output" />
    <figcaption>
      Object Editing (project demo).
      The blue windmill blades are modified between input and output,
      but the flower background and camera feel stay consistent.
      GS-DiT uses its pseudo 4D field + diffusion to rewrite only the targeted object.
    </figcaption>
  </figure>

  <h3>4.4 Multi-camera shooting from one clip</h3>
  <p>
    This is the “film studio” moment.
    Give GS-DiT a single input clip (for example, a drone-like shot over a coastline).
    It will generate several different camera trajectories for that same moment:
    rotate left, rotate right, tilt down-left, etc.
    That means you can cut between angles that were never actually recorded.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/multi_camera.png"
         alt="Multi-camera shooting: one coastal clip and three new trajectories" />
    <figcaption>
      Multi-camera Shooting (project demo).
      Far left: original clip of waves hitting the shore.
      Next columns: GS-DiT’s re-shoots with three different “virtual” camera motions.
      It looks like you had multiple cameras in the air at once,
      but in reality you had only one video.
    </figcaption>
  </figure>


  <!-- 5. Quantitative and qualitative comparison -->
  <h2>5. How does GS-DiT compare to previous work?</h2>

  <p>
    GS-DiT is not just a style filter; it’s competitive (or better) on actual 3D tracking
    and multi-view video generation benchmarks.
    The paper compares against strong baselines like MonST3R and GCD.
    Qualitatively, competing systems often distort shapes, tear objects apart,
    or leave black voids where the scene is missing.
    GS-DiT tends to keep appearance intact and produce more complete novel views.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure4.png"
         alt="Figure 4 from the paper: qualitative comparison with baselines for new-view generation" />
    <figcaption>
      Figure 4 (Bian et al., 2025). Qualitative comparison on several scenes:
      swan, biker, car, cow, old man, cartoon otter, etc.
      Columns show: input frames, recovered camera poses, GS-DiT output,
      and baselines (GCD, MonST3R).
      GS-DiT’s results look more complete and less “broken,” especially
      in background regions and along object boundaries.
    </figcaption>
  </figure>

  <p>
    They also report standard tracking metrics.
    Table&nbsp;1 and Table&nbsp;2 compare 2D and 3D point tracking quality on TAPVid
    and related splits. Higher AJ / OA means better tracking, and lower APD means
    more precise 3D alignment. GS-DiT’s dense 3D tracking module is very strong —
    close to or better than prior approaches, even when combined with external depth estimators.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/table1and2.png"
         alt="Table 1 and Table 2 from the paper: comparison of 2D and 3D point tracking metrics" />
    <figcaption>
      Table&nbsp;1 &amp; 2 (Bian et al., 2025). Tracking metrics.
      GS-DiT’s tracking (with depth) gives competitive or better accuracy in both 2D
      and 3D settings. Strong tracking is what later enables stable camera control.
    </figcaption>
  </figure>

  <p>
    Finally, Table&nbsp;3 shows video quality metrics (PSNR↑, SSIM↑, LPIPS↓) for
    multi-shooting generation — i.e. producing several camera trajectories from one clip.
    GS-DiT scores the best overall across datasets like DAVIS, “Sora”-style data, and Pixabay.
    In plain English: not only does it “look right,” it is numerically closer to ground truth
    and perceptually sharper than previous methods.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/table3.png"
         alt="Table 3 from the paper: quantitative comparison of multi-shooting video generation quality" />
    <figcaption>
      Table&nbsp;3 (Bian et al., 2025). Quantitative comparison for the “multi-camera shooting”
      task. GS-DiT achieves higher PSNR / SSIM and lower LPIPS (better perceptual similarity)
      than GCD and MonST3R across multiple datasets.
    </figcaption>
  </figure>


  <!-- 6. Limitations -->
  <h2>6. Limitations and open problems</h2>

  <p>
    The authors are honest that GS-DiT is not perfect:
  </p>

  <ul>
    <li>
      <b>Extreme unseen viewpoints</b> – if you ask for a camera angle that reveals
      surfaces never visible in the input video, GS-DiT has to hallucinate.
      Sometimes that guess is wrong (texture smears, geometry glitches).
    </li>

    <li>
      <b>Very thin / complex geometry</b> – detailed structures like hair,
      thin branches, reflections on glass, etc. are still challenging to track
      and reconstruct stably in full 3D.
    </li>

    <li>
      <b>Compute cost</b> – generating a whole novel camera sequence is heavier
      than a simple 2D filter. The pipeline needs dense 3D point tracking,
      pseudo 4D Gaussian rendering, and Transformer diffusion.  
      It’s way more “virtual production” than “Instagram filter.”
    </li>
  </ul>

  <p>
    Figure&nbsp;5 below (from the paper) shows examples of GS-DiT controlling camera pose,
    editing objects, doing dolly zoom, etc. The important part is consistency:
    shadows, lighting, and global layout keep making sense while the viewpoint changes.
  </p>

  <figure class="wide-figure">
    <img src="assets/GS-DiT/figure5.png"
         alt="Figure 5 from the paper: examples of pose control, dolly zoom, object editing, multi-camera shooting" />
    <figcaption>
      Figure 5 (Bian et al., 2025). Examples of the key abilities:
      (a) camera pose control, (b) dolly zoom, (c) object editing,
      (d) multi-camera shooting. GS-DiT keeps the scene coherent
      while applying cinematic camera moves or edits that were never filmed.
    </figcaption>
  </figure>


  <!-- 7. Takeaways -->
  <h2>7. Takeaways</h2>

  <p>
    GS-DiT treats a regular monocular video not as “a clip,” but as
    raw data to reconstruct a controllable little 4D world.
    After tracking everything in 3D and building a pseudo 4D Gaussian field,
    it asks a DiT-style diffusion model to render that world from any camera path
    we like, or with certain objects edited.
  </p>

  <p>
    Compared to earlier diffusion video models, the big jump is
    <b>3D-aware control</b>.
    Instead of “please make it look cool,” we can say
    “move the camera 10 degrees left and tilt down,”
    and get a stable, realistic output shot.
    That’s a huge step toward turning casual phone videos into
    editable, cinematic 3D scenes — basically virtual cinematography
    from whatever you happened to record.
  </p>


  <footer>
    <h3>References / figure credits</h3>

    <p>
      Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., and Li, H.
      “<i>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking.</i>”
      arXiv:2501.02690, 2025.
    </p>

    <p>
      Peebles, W., &amp; Xie, S.
      “<i>Scalable Diffusion Models with Transformers.</i>”
      arXiv:2212.09748, 2022.
      (This work introduced DiT, a Transformer-based latent diffusion backbone
      that GS-DiT builds on.)
    </p>

    <p>
      All figures (Figure&nbsp;1–5, Tables&nbsp;1–3) and demo frames / videos
      are from Bian et al. (2025)’s paper and official project page (GS-DiT).
      DiT block diagram is adapted from Peebles &amp; Xie (2022).
    </p>
  </footer>

</body>
</html>
